#!/usr/bin/env node
/**
 * File Upload Pipeline for Headhunter AI
 * Handles resume uploads, text extraction, and triggers processing pipeline
 */

import { onCall, HttpsError } from "firebase-functions/v2/https";
import { onObjectFinalized } from "firebase-functions/v2/storage";
import * as admin from "firebase-admin";
import { z } from "zod";
import { Storage } from "@google-cloud/storage";
import { BUCKET_FILES } from "./config";
import * as path from "path";

// Initialize services
const firestore = admin.firestore();
const storage = new Storage();

// For text extraction (would need to add these dependencies)
// import * as pdfParse from 'pdf-parse';
// import * as mammoth from 'mammoth';
// import * as vision from '@google-cloud/vision';

// Validation Schemas
const UploadResumeSchema = z.object({
  candidate_id: z.string().min(1).max(100),
  file_name: z.string().min(1).max(255),
  file_size: z.number().min(1).max(50 * 1024 * 1024), // 50MB max
  content_type: z.string().min(1),
  metadata: z.object({
    original_name: z.string().optional(),
    uploaded_by: z.string().optional(),
    tags: z.array(z.string()).optional(),
  }).optional(),
});

const ProcessFileSchema = z.object({
  file_path: z.string().min(1),
  candidate_id: z.string().min(1).max(100),
  file_type: z.enum(['pdf', 'docx', 'txt', 'doc', 'rtf', 'jpg', 'png', 'jpeg']),
  processing_options: z.object({
    extract_text: z.boolean().default(true),
    ocr_enabled: z.boolean().default(true),
    auto_trigger_analysis: z.boolean().default(true),
  }).optional(),
});

// Helper Functions
function validateAuth(request: any): { userId: string; orgId: string } {
  if (!request.auth) {
    throw new HttpsError("unauthenticated", "Authentication required");
  }
  
  const userId = request.auth.uid;
  const orgId = request.auth.token.org_id;
  
  if (!orgId) {
    throw new HttpsError("permission-denied", "Organization membership required");
  }
  
  return { userId, orgId };
}

async function validatePermissions(userId: string, permission: string): Promise<boolean> {
  const userDoc = await firestore.collection('users').doc(userId).get();
  
  if (!userDoc.exists) {
    throw new HttpsError("not-found", "User profile not found");
  }
  
  const userData = userDoc.data();
  return userData?.permissions?.[permission] === true;
}

function getFileExtension(fileName: string): string {
  return path.extname(fileName).toLowerCase().replace('.', '');
}

function isValidFileType(fileName: string): boolean {
  const validExtensions = ['pdf', 'docx', 'doc', 'txt', 'rtf', 'jpg', 'jpeg', 'png'];
  const extension = getFileExtension(fileName);
  return validExtensions.includes(extension);
}

function sanitizeFileName(fileName: string): string {
  // Remove or replace invalid characters
  return fileName
    .replace(/[^a-zA-Z0-9.-_]/g, '_')
    .replace(/_{2,}/g, '_')
    .toLowerCase();
}

/**
 * Generate signed upload URL for client-side uploads
 */
export const generateUploadUrl = onCall(
  {
    memory: "256MiB",
    timeoutSeconds: 30,
  },
  async (request) => {
    const { userId, orgId } = validateAuth(request);

    // Check permissions
    const canUpload = await validatePermissions(userId, 'can_edit_candidates');
    if (!canUpload) {
      throw new HttpsError("permission-denied", "Insufficient permissions to upload files");
    }

    // Validate input
    let validatedInput;
    try {
      validatedInput = UploadResumeSchema.parse(request.data);
    } catch (error) {
      if (error instanceof z.ZodError) {
        throw new HttpsError("invalid-argument", `Invalid input: ${error.errors[0].message}`);
      }
      throw new HttpsError("invalid-argument", "Invalid request data");
    }

    const { candidate_id, file_name, file_size, content_type, metadata } = validatedInput;

    try {
      // Validate file type
      if (!isValidFileType(file_name)) {
        throw new HttpsError("invalid-argument", "Invalid file type. Supported: PDF, DOCX, DOC, TXT, RTF, JPG, PNG");
      }

      // Check if candidate exists and user has access
      const candidateDoc = await firestore.collection('candidates').doc(candidate_id).get();
      if (!candidateDoc.exists) {
        throw new HttpsError("not-found", "Candidate not found");
      }

      const candidateData = candidateDoc.data();
      if (candidateData?.org_id !== orgId) {
        throw new HttpsError("permission-denied", "Access denied to this candidate");
      }

      // Generate unique file path
      const fileExtension = getFileExtension(file_name);
      const sanitizedFileName = sanitizeFileName(file_name);
      const timestamp = Date.now();
      const filePath = `organizations/${orgId}/candidates/${candidate_id}/resumes/${timestamp}_${sanitizedFileName}`;

      // Generate signed URL for upload
      const bucket = storage.bucket(BUCKET_FILES);
      const file = bucket.file(filePath);

      const [signedUrl] = await file.getSignedUrl({
        version: 'v4',
        action: 'write',
        expires: Date.now() + 15 * 60 * 1000, // 15 minutes
        contentType: content_type,
        extensionHeaders: {
          'x-goog-meta-candidate-id': candidate_id,
          'x-goog-meta-org-id': orgId,
          'x-goog-meta-uploaded-by': userId,
          'x-goog-meta-original-name': file_name,
          'x-goog-meta-file-type': fileExtension,
        },
      });

      // Store upload session info
      const uploadSessionId = `upload_${timestamp}_${candidate_id}`;
      await firestore.collection('upload_sessions').doc(uploadSessionId).set({
        candidate_id,
        org_id: orgId,
        uploaded_by: userId,
        file_path: filePath,
        original_file_name: file_name,
        file_size,
        content_type,
        file_extension: fileExtension,
        status: 'pending_upload',
        created_at: admin.firestore.FieldValue.serverTimestamp(),
        expires_at: new Date(Date.now() + 15 * 60 * 1000),
        metadata: metadata || {},
      });

      return {
        success: true,
        upload_url: signedUrl,
        upload_session_id: uploadSessionId,
        file_path: filePath,
        expires_in_minutes: 15,
      };
    } catch (error) {
      if (error instanceof HttpsError) throw error;
      console.error("Error generating upload URL:", error);
      throw new HttpsError("internal", "Failed to generate upload URL");
    }
  }
);

/**
 * Confirm upload completion and trigger processing
 */
export const confirmUpload = onCall(
  {
    memory: "256MiB",
    timeoutSeconds: 60,
  },
  async (request) => {
    const { userId, orgId } = validateAuth(request);
    const { upload_session_id } = request.data;

    if (!upload_session_id) {
      throw new HttpsError("invalid-argument", "Upload session ID is required");
    }

    try {
      // Get upload session
      const sessionDoc = await firestore.collection('upload_sessions').doc(upload_session_id).get();
      
      if (!sessionDoc.exists) {
        throw new HttpsError("not-found", "Upload session not found");
      }

      const sessionData = sessionDoc.data();

      // Validate access
      if (sessionData?.org_id !== orgId || sessionData?.uploaded_by !== userId) {
        throw new HttpsError("permission-denied", "Access denied to this upload session");
      }

      // Check if file exists in storage
      const bucket = storage.bucket(BUCKET_FILES);
      const file = bucket.file(sessionData.file_path);
      const [exists] = await file.exists();

      if (!exists) {
        throw new HttpsError("not-found", "File not found in storage");
      }

      // Get file metadata
      const [metadata] = await file.getMetadata();
      const fileSize = parseInt(String(metadata.size || '0'));

      // Update candidate with file information
      const candidateUpdateData = {
        'documents.resume_file_url': sessionData.file_path,
        'documents.resume_file_name': sessionData.original_file_name,
        'documents.resume_file_size': fileSize,
        'documents.resume_uploaded_at': admin.firestore.FieldValue.serverTimestamp(),
        'processing.status': 'awaiting_text_extraction',
        updated_at: admin.firestore.FieldValue.serverTimestamp(),
      };

      await firestore.collection('candidates').doc(sessionData.candidate_id).update(candidateUpdateData);

      // Add to processing queue for text extraction
      await firestore.collection('processing_queue').add({
        candidate_id: sessionData.candidate_id,
        org_id: orgId,
        stage: 'text_extraction',
        file_info: {
          file_path: sessionData.file_path,
          original_name: sessionData.original_file_name,
          file_type: sessionData.file_extension,
          file_size: fileSize,
          content_type: sessionData.content_type,
        },
        processing_options: {
          extract_text: true,
          ocr_enabled: ['jpg', 'jpeg', 'png'].includes(sessionData.file_extension),
          auto_trigger_analysis: true,
        },
        created_at: admin.firestore.FieldValue.serverTimestamp(),
        status: 'queued',
      });

      // Update upload session
      await sessionDoc.ref.update({
        status: 'completed',
        completed_at: admin.firestore.FieldValue.serverTimestamp(),
        file_size_actual: fileSize,
      });

      return {
        success: true,
        candidate_id: sessionData.candidate_id,
        file_path: sessionData.file_path,
        processing_queued: true,
      };
    } catch (error) {
      if (error instanceof HttpsError) throw error;
      console.error("Error confirming upload:", error);
      throw new HttpsError("internal", "Failed to confirm upload");
    }
  }
);

/**
 * Storage trigger for automatic file processing
 */
export const processUploadedFile = onObjectFinalized(
  {
    bucket: "headhunter-ai-0088-files",
    memory: "2GiB",
    timeoutSeconds: 540, // 9 minutes
    retry: true,
  },
  async (event) => {
    const filePath = event.data.name;
    const bucket = event.data.bucket;

    console.log(`Processing uploaded file: ${filePath} in bucket: ${bucket}`);

    // Only process files in the organizations/*/candidates/*/resumes/ path
    const pathParts = filePath.split('/');
    if (pathParts.length < 6 || pathParts[0] !== 'organizations' || pathParts[2] !== 'candidates' || pathParts[4] !== 'resumes') {
      console.log(`Skipping file: ${filePath} - not a resume file`);
      return;
    }

    const orgId = pathParts[1];
    const candidateId = pathParts[3];

    try {
      // Get file metadata
      const file = storage.bucket(bucket).file(filePath);
      const [metadata] = await file.getMetadata();
      
      const candidateIdFromMeta = metadata.metadata?.['candidate-id'];
      const orgIdFromMeta = metadata.metadata?.['org-id'];
      const fileType = String(metadata.metadata?.['file-type'] || getFileExtension(filePath));

      // Validate metadata matches path
      if (candidateIdFromMeta !== candidateId || orgIdFromMeta !== orgId) {
        console.error(`Metadata mismatch for file ${filePath}`);
        return;
      }

      // Download and process file
      const [fileBuffer] = await file.download();
      let extractedText = '';
      let extractionMethod = '';

      try {
        switch (fileType.toLowerCase()) {
          case 'pdf':
            extractedText = await extractPdfText(fileBuffer);
            extractionMethod = 'pdf-parser';
            break;
          case 'docx':
            extractedText = await extractDocxText(fileBuffer);
            extractionMethod = 'mammoth';
            break;
          case 'doc':
            extractedText = await extractDocText(fileBuffer);
            extractionMethod = 'antiword-fallback';
            break;
          case 'txt':
          case 'rtf':
            extractedText = fileBuffer.toString('utf-8');
            extractionMethod = 'utf8-decode';
            break;
          case 'jpg':
          case 'jpeg':
          case 'png':
            extractedText = await extractImageText(fileBuffer);
            extractionMethod = 'ocr-vision-api';
            break;
          default:
            console.warn(`Unsupported file type: ${fileType}`);
            return;
        }
      } catch (extractionError) {
        console.error(`Text extraction failed for ${filePath}:`, extractionError);
        
        // Update candidate with extraction failure
        await firestore.collection('candidates').doc(candidateId).update({
          'processing.status': 'extraction_failed',
          'processing.error_message': `Text extraction failed: ${(extractionError as Error).message}`,
          'processing.last_processed': admin.firestore.FieldValue.serverTimestamp(),
          updated_at: admin.firestore.FieldValue.serverTimestamp(),
        });
        
        return;
      }

      // Clean and validate extracted text
      const cleanText = cleanExtractedText(extractedText);
      
      if (!cleanText || cleanText.length < 50) {
        console.warn(`Insufficient text extracted from ${filePath}: ${cleanText.length} characters`);
        
        await firestore.collection('candidates').doc(candidateId).update({
          'processing.status': 'extraction_insufficient',
          'processing.error_message': 'Insufficient text extracted from document',
          updated_at: admin.firestore.FieldValue.serverTimestamp(),
        });
        
        return;
      }

      // Update candidate with extracted text
      await firestore.collection('candidates').doc(candidateId).update({
        'documents.resume_text': cleanText,
        'documents.text_extraction_method': extractionMethod,
        'documents.text_extracted_at': admin.firestore.FieldValue.serverTimestamp(),
        'processing.status': 'text_extracted',
        'processing.local_analysis_completed': false,
        updated_at: admin.firestore.FieldValue.serverTimestamp(),
      });

      // Add to processing queue for LLM analysis
      await firestore.collection('processing_queue').add({
        candidate_id: candidateId,
        org_id: orgId,
        stage: 'awaiting_analysis',
        structured_data: {
          candidate_id: candidateId,
          raw_text: cleanText,
          extracted_at: new Date().toISOString(),
          extraction_method: extractionMethod,
          file_metadata: {
            filename: metadata.name,
            size: metadata.size,
            content_type: metadata.contentType,
            original_name: metadata.metadata?.['original-name'],
          },
        },
        created_at: admin.firestore.FieldValue.serverTimestamp(),
        status: 'queued',
      });

      console.log(`Successfully processed file for candidate: ${candidateId}, extracted ${cleanText.length} characters`);

    } catch (error) {
      console.error(`Error processing file ${filePath}:`, error);
      
      // Update candidate with processing failure
      try {
        await firestore.collection('candidates').doc(candidateId).update({
          'processing.status': 'processing_failed',
          'processing.error_message': (error as Error).message,
          'processing.last_processed': admin.firestore.FieldValue.serverTimestamp(),
          updated_at: admin.firestore.FieldValue.serverTimestamp(),
        });
      } catch (updateError) {
        console.error(`Failed to update candidate ${candidateId} with error status:`, updateError);
      }
      
      throw error; // This will trigger retry if retry is enabled
    }
  }
);

/**
 * Manual file processing endpoint for testing/debugging
 */
export const processFile = onCall(
  {
    memory: "2GiB",
    timeoutSeconds: 300,
  },
  async (request) => {
    const { userId, orgId } = validateAuth(request);

    // Check permissions
    const canProcess = await validatePermissions(userId, 'can_edit_candidates');
    if (!canProcess) {
      throw new HttpsError("permission-denied", "Insufficient permissions to process files");
    }

    // Validate input
    let validatedInput;
    try {
      validatedInput = ProcessFileSchema.parse(request.data);
    } catch (error) {
      if (error instanceof z.ZodError) {
        throw new HttpsError("invalid-argument", `Invalid input: ${error.errors[0].message}`);
      }
      throw new HttpsError("invalid-argument", "Invalid request data");
    }

    const { file_path, candidate_id, file_type, processing_options } = validatedInput;

    try {
      // Verify candidate access
      const candidateDoc = await firestore.collection('candidates').doc(candidate_id).get();
      if (!candidateDoc.exists) {
        throw new HttpsError("not-found", "Candidate not found");
      }

      const candidateData = candidateDoc.data();
      if (candidateData?.org_id !== orgId) {
        throw new HttpsError("permission-denied", "Access denied to this candidate");
      }

      // Download file
      const bucket = storage.bucket(BUCKET_FILES);
      const file = bucket.file(file_path);
      
      const [exists] = await file.exists();
      if (!exists) {
        throw new HttpsError("not-found", "File not found in storage");
      }

      const [fileBuffer] = await file.download();
      
      let result = {
        success: true,
        candidate_id,
        file_path,
        extracted_text: '',
        extraction_method: '',
        text_length: 0,
      };

      // Extract text if requested
      if (processing_options?.extract_text !== false) {
        let extractedText = '';
        
        switch (file_type.toLowerCase()) {
          case 'pdf':
            extractedText = await extractPdfText(fileBuffer);
            result.extraction_method = 'pdf-parser';
            break;
          case 'docx':
            extractedText = await extractDocxText(fileBuffer);
            result.extraction_method = 'mammoth';
            break;
          case 'txt':
            extractedText = fileBuffer.toString('utf-8');
            result.extraction_method = 'utf8-decode';
            break;
          default:
            throw new HttpsError("invalid-argument", `Unsupported file type: ${file_type}`);
        }

        const cleanText = cleanExtractedText(extractedText);
        result.extracted_text = cleanText;
        result.text_length = cleanText.length;

        // Update candidate if auto-trigger is enabled
        if (processing_options?.auto_trigger_analysis !== false) {
          await firestore.collection('candidates').doc(candidate_id).update({
            'documents.resume_text': cleanText,
            'documents.text_extraction_method': result.extraction_method,
            'documents.text_extracted_at': admin.firestore.FieldValue.serverTimestamp(),
            'processing.status': 'text_extracted',
            updated_at: admin.firestore.FieldValue.serverTimestamp(),
          });

          // Add to analysis queue
          await firestore.collection('processing_queue').add({
            candidate_id,
            org_id: orgId,
            stage: 'awaiting_analysis',
            structured_data: {
              candidate_id,
              raw_text: cleanText,
              extracted_at: new Date().toISOString(),
              extraction_method: result.extraction_method,
            },
            created_at: admin.firestore.FieldValue.serverTimestamp(),
            status: 'queued',
          });

          (result as any).analysis_queued = true;
        }
      }

      return result;
    } catch (error) {
      if (error instanceof HttpsError) throw error;
      console.error("Error processing file:", error);
      throw new HttpsError("internal", "Failed to process file");
    }
  }
);

/**
 * Delete uploaded file
 */
export const deleteFile = onCall(
  {
    memory: "256MiB",
    timeoutSeconds: 60,
  },
  async (request) => {
    const { userId, orgId } = validateAuth(request);
    const { candidate_id, file_path } = request.data;

    if (!candidate_id || !file_path) {
      throw new HttpsError("invalid-argument", "Candidate ID and file path are required");
    }

    // Check permissions
    const canDelete = await validatePermissions(userId, 'can_edit_candidates');
    if (!canDelete) {
      throw new HttpsError("permission-denied", "Insufficient permissions to delete files");
    }

    try {
      // Verify candidate access
      const candidateDoc = await firestore.collection('candidates').doc(candidate_id).get();
      if (!candidateDoc.exists) {
        throw new HttpsError("not-found", "Candidate not found");
      }

      const candidateData = candidateDoc.data();
      if (candidateData?.org_id !== orgId) {
        throw new HttpsError("permission-denied", "Access denied to this candidate");
      }

      // Delete file from storage
      const bucket = storage.bucket(BUCKET_FILES);
      const file = bucket.file(file_path);
      
      const [exists] = await file.exists();
      if (exists) {
        await file.delete();
      }

      // Update candidate document
      await candidateDoc.ref.update({
        'documents.resume_file_url': admin.firestore.FieldValue.delete(),
        'documents.resume_file_name': admin.firestore.FieldValue.delete(),
        'documents.resume_text': admin.firestore.FieldValue.delete(),
        'documents.text_extraction_method': admin.firestore.FieldValue.delete(),
        'documents.text_extracted_at': admin.firestore.FieldValue.delete(),
        'processing.status': 'pending',
        'processing.local_analysis_completed': false,
        updated_at: admin.firestore.FieldValue.serverTimestamp(),
      });

      // Clean up processing queue entries
      const processingQuery = await firestore
        .collection('processing_queue')
        .where('candidate_id', '==', candidate_id)
        .get();

      const batch = firestore.batch();
      processingQuery.docs.forEach(doc => {
        batch.delete(doc.ref);
      });
      await batch.commit();

      return {
        success: true,
        candidate_id,
        file_path,
        deleted: true,
      };
    } catch (error) {
      if (error instanceof HttpsError) throw error;
      console.error("Error deleting file:", error);
      throw new HttpsError("internal", "Failed to delete file");
    }
  }
);

// Text Extraction Helper Functions

async function extractPdfText(buffer: Buffer): Promise<string> {
  // This would use pdf-parse library
  // const pdfParse = require('pdf-parse');
  // const data = await pdfParse(buffer);
  // return data.text;
  
  // Placeholder implementation
  return "PDF text extraction not implemented - would use pdf-parse library";
}

async function extractDocxText(buffer: Buffer): Promise<string> {
  // This would use mammoth library
  // const mammoth = require('mammoth');
  // const result = await mammoth.extractRawText({ buffer });
  // return result.value;
  
  // Placeholder implementation
  return "DOCX text extraction not implemented - would use mammoth library";
}

async function extractDocText(buffer: Buffer): Promise<string> {
  // This would use antiword or similar for .doc files
  // More complex implementation required
  
  // Placeholder implementation
  return "DOC text extraction not implemented - would use antiword or similar";
}

async function extractImageText(buffer: Buffer): Promise<string> {
  // This would use Google Cloud Vision API
  // const vision = require('@google-cloud/vision');
  // const client = new vision.ImageAnnotatorClient();
  // const [result] = await client.textDetection({ image: { content: buffer } });
  // const detections = result.textAnnotations;
  // return detections && detections.length > 0 ? detections[0].description : '';
  
  // Placeholder implementation
  return "OCR text extraction not implemented - would use Google Cloud Vision API";
}

function cleanExtractedText(text: string): string {
  return text
    .trim()
    .replace(/\r\n/g, '\n')
    .replace(/\r/g, '\n')
    .replace(/\n{3,}/g, '\n\n')
    .replace(/\s{2,}/g, ' ')
    .replace(/[^\x20-\x7E\n]/g, '') // Remove non-printable characters
    .substring(0, 100000); // Limit to 100k characters
}

/**
 * Get file upload statistics
 */
export const getUploadStats = onCall(
  {
    memory: "512MiB",
    timeoutSeconds: 60,
  },
  async (request) => {
    const { userId, orgId } = validateAuth(request);

    // Check permissions
    const canView = await validatePermissions(userId, 'can_view_candidates');
    if (!canView) {
      throw new HttpsError("permission-denied", "Insufficient permissions to view statistics");
    }

    try {
      const candidatesRef = firestore.collection('candidates').where('org_id', '==', orgId);

      // Get candidates with files
      const withFilesSnapshot = await candidatesRef
        .where('documents.resume_file_url', '!=', null)
        .count()
        .get();
      const candidatesWithFiles = withFilesSnapshot.data().count;

      // Get candidates with extracted text
      const withTextSnapshot = await candidatesRef
        .where('documents.resume_text', '!=', null)
        .count()
        .get();
      const candidatesWithText = withTextSnapshot.data().count;

      // Get processing statistics
      const processingQuery = await firestore
        .collection('processing_queue')
        .where('org_id', '==', orgId)
        .get();

      const processingStats = {
        total_in_queue: processingQuery.size,
        by_stage: {},
        by_status: {},
      };

      processingQuery.docs.forEach(doc => {
        const data = doc.data();
        const stage = data.stage || 'unknown';
        const status = data.status || 'unknown';
        
        (processingStats.by_stage as any)[stage] = ((processingStats.by_stage as any)[stage] || 0) + 1;
        (processingStats.by_status as any)[status] = ((processingStats.by_status as any)[status] || 0) + 1;
      });

      return {
        success: true,
        stats: {
          candidates_with_files: candidatesWithFiles,
          candidates_with_extracted_text: candidatesWithText,
          text_extraction_rate: candidatesWithFiles > 0 
            ? Math.round((candidatesWithText / candidatesWithFiles) * 100)
            : 0,
          processing_queue: processingStats,
        },
      };
    } catch (error) {
      console.error("Error getting upload stats:", error);
      throw new HttpsError("internal", "Failed to retrieve statistics");
    }
  }
);
