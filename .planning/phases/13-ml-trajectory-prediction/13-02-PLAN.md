---
phase: 13-ml-trajectory-prediction
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/ml-training/trajectory/data_preparation.py
  - scripts/ml-training/trajectory/title_encoder.py
  - scripts/ml-training/trajectory/lstm_model.py
  - scripts/ml-training/trajectory/train.py
  - scripts/ml-training/trajectory/export_onnx.py
  - scripts/ml-training/trajectory/calibrate.py
  - scripts/ml-training/trajectory/evaluate.py
  - scripts/ml-training/requirements.txt
autonomous: true

must_haves:
  truths:
    - "Training script can load career sequences from PostgreSQL"
    - "LSTM model trains without errors on sample data"
    - "Model exports to ONNX format with dynamic sequence lengths"
    - "Confidence calibrator can be trained on validation set"
  artifacts:
    - path: "scripts/ml-training/trajectory/lstm_model.py"
      provides: "PyTorch LSTM model definition"
      contains: "class CareerTrajectoryLSTM"
    - path: "scripts/ml-training/trajectory/train.py"
      provides: "Training loop with time-aware splits"
      contains: "temporal_split"
    - path: "scripts/ml-training/trajectory/export_onnx.py"
      provides: "ONNX export with dynamic shapes"
      contains: "dynamic_shapes"
    - path: "scripts/ml-training/trajectory/calibrate.py"
      provides: "Isotonic regression calibrator"
      contains: "IsotonicRegression"
  key_links:
    - from: "scripts/ml-training/trajectory/train.py"
      to: "scripts/ml-training/trajectory/lstm_model.py"
      via: "import"
      pattern: "from lstm_model import CareerTrajectoryLSTM"
    - from: "scripts/ml-training/trajectory/export_onnx.py"
      to: "ONNX format"
      via: "torch.onnx.export"
      pattern: "torch\\.onnx\\.export.*dynamo=True"
---

<objective>
Create Python ML training pipeline for career trajectory LSTM model

Purpose: Build the offline training infrastructure to create the LSTM model that will power next role, tenure, and hireability predictions. Uses time-aware data splits to prevent temporal leakage.

Output: Complete Python training pipeline that produces trajectory-lstm.onnx and calibration artifacts.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-ml-trajectory-prediction/13-RESEARCH.md
@services/hh-search-svc/src/trajectory-calculators.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create data preparation and title encoder modules</name>
  <files>
    scripts/ml-training/trajectory/data_preparation.py
    scripts/ml-training/trajectory/title_encoder.py
    scripts/ml-training/requirements.txt
  </files>
  <action>
Create data preparation infrastructure:

1. Create `scripts/ml-training/requirements.txt`:
   ```
   torch>=2.5.0
   onnx>=1.15.0
   onnx-simplifier>=0.4.35
   scikit-learn>=1.4.0
   pandas>=2.2.0
   psycopg2-binary>=2.9.9
   numpy>=1.26.0
   tqdm>=4.66.0
   python-dotenv>=1.0.0
   ```

2. Create `scripts/ml-training/trajectory/data_preparation.py`:
   - Function `load_career_sequences(db_url: str) -> pd.DataFrame`:
     - Connect to PostgreSQL using psycopg2
     - Query candidates table for id, work history JSON
     - Extract title sequences in chronological order (oldest first)
     - Filter candidates with 2+ roles
   - Function `temporal_split(df, train_cutoff, val_cutoff)`:
     - Split by latest experience end date (NOT random)
     - Return train/val/test DataFrames
     - CRITICAL: Split BEFORE generating input-output pairs to prevent temporal leakage
   - Function `generate_sequences(df) -> List[Tuple]`:
     - Create (input_titles, next_title, tenure_months, candidate_id) tuples
     - For each career: titles[:-1] predicts titles[-1]
   - Include progress bar with tqdm

3. Create `scripts/ml-training/trajectory/title_encoder.py`:
   - Class `TitleEncoder`:
     - `fit(titles: List[str])` - Build vocabulary from training titles
     - `encode(title: str) -> int` - Return index (0 for unknown)
     - `decode(index: int) -> str` - Return title string
     - `save(path: str)` - Save vocab to JSON
     - `load(path: str)` - Load vocab from JSON
   - Normalize titles: lowercase, strip, remove punctuation
   - Reserve index 0 for unknown/padding
   - Map common variations (Sr. -> Senior, Jr. -> Junior)
  </action>
  <verify>
    - `python3 -c "import scripts.ml_training.trajectory.data_preparation; print('OK')"`
    - `python3 -c "from scripts.ml_training.trajectory.title_encoder import TitleEncoder; t = TitleEncoder(); t.fit(['Engineer', 'Senior Engineer']); print(t.encode('engineer'))"`
  </verify>
  <done>Data preparation loads career sequences, title encoder normalizes and encodes titles</done>
</task>

<task type="auto">
  <name>Task 2: Create LSTM model definition and training loop</name>
  <files>
    scripts/ml-training/trajectory/lstm_model.py
    scripts/ml-training/trajectory/train.py
  </files>
  <action>
Create PyTorch model and training infrastructure:

1. Create `scripts/ml-training/trajectory/lstm_model.py`:
   - Class `CareerTrajectoryLSTM(nn.Module)`:
     - `__init__(vocab_size, embedding_dim=128, hidden_dim=16, num_layers=2, dropout=0.1)`:
       - Use BIDIRECTIONAL LSTM per RESEARCH.md (outperforms GRU)
       - hidden_dim=16 is research-validated for career prediction
       - Embedding layer with padding_idx=0
       - Three output heads:
         - `classifier`: Linear -> ReLU -> Dropout -> Linear (num_classes)
         - `tenure_head`: Linear -> ReLU -> Linear(2) for min/max months
         - `hireability_head`: Linear -> ReLU -> Linear(1) -> Sigmoid
     - `forward(title_ids, lengths=None)`:
       - Handle variable sequence lengths with pack_padded_sequence
       - Return (next_role_logits, tenure_pred, hireability)
       - Concatenate forward/backward hidden states

2. Create `scripts/ml-training/trajectory/train.py`:
   - Function `create_dataloaders(train_df, val_df, encoder, batch_size=32)`:
     - Custom Dataset class for career sequences
     - Collate function with padding for variable lengths
   - Function `train_epoch(model, dataloader, optimizer, criterion)`:
     - Combined loss: CrossEntropy for next role + MSE for tenure + BCE for hireability
     - Return average loss
   - Function `evaluate(model, dataloader)`:
     - Return accuracy, loss, predictions for calibration
   - Main training loop:
     - Adam optimizer with lr=1e-3
     - 50 epochs with early stopping (patience=5)
     - Save best model checkpoint
     - Log training progress
   - CLI: `python train.py --db-url $POSTGRES_URL --epochs 50 --output models/`
  </action>
  <verify>
    - `python3 -c "from scripts.ml_training.trajectory.lstm_model import CareerTrajectoryLSTM; m = CareerTrajectoryLSTM(vocab_size=100); print(m)"`
    - `python3 scripts/ml-training/trajectory/train.py --help` (should show CLI options)
  </verify>
  <done>LSTM model trains on career sequences with multi-task output heads</done>
</task>

<task type="auto">
  <name>Task 3: Create ONNX export and calibration modules</name>
  <files>
    scripts/ml-training/trajectory/export_onnx.py
    scripts/ml-training/trajectory/calibrate.py
    scripts/ml-training/trajectory/evaluate.py
  </files>
  <action>
Create export and calibration infrastructure:

1. Create `scripts/ml-training/trajectory/export_onnx.py`:
   - Function `export_to_onnx(model_path, vocab_path, output_path)`:
     - Load trained PyTorch model
     - Create example inputs with batch=1, seq_len=10
     - Export with `torch.onnx.export(dynamo=True)` per RESEARCH.md
     - Use dynamic_shapes for batch and seq_len dimensions:
       ```python
       dynamic_shapes={
           'title_ids': {0: torch.export.Dim('batch'), 1: torch.export.Dim('seq_len')},
           'lengths': {0: torch.export.Dim('batch')}
       }
       ```
     - Use opset_version=17
     - Input names: ['title_ids', 'lengths']
     - Output names: ['next_role_logits', 'tenure_pred', 'hireability']
   - Function `simplify_onnx(model_path)`:
     - Use onnx-simplifier for optimization
     - Save simplified model
   - CLI: `python export_onnx.py --checkpoint models/best.pt --vocab models/vocab.json --output models/trajectory-lstm.onnx`

2. Create `scripts/ml-training/trajectory/calibrate.py`:
   - Function `train_calibrator(model, val_loader, device)`:
     - Collect raw confidence scores and correctness labels
     - Train IsotonicRegression(out_of_bounds='clip')
     - Return fitted calibrator
   - Function `save_calibrator(calibrator, path)`:
     - Pickle calibrator to file
   - Function `evaluate_calibration(calibrator, raw_probs, labels)`:
     - Compute ECE (Expected Calibration Error)
     - Target: ECE < 0.05 per RESEARCH.md
     - Return metrics dict
   - CLI: `python calibrate.py --checkpoint models/best.pt --data-path data/ --output models/calibrator.pkl`

3. Create `scripts/ml-training/trajectory/evaluate.py`:
   - Function `evaluate_model(model_path, test_loader)`:
     - Compute next role accuracy (top-1 and top-5)
     - Compute tenure MAE (mean absolute error)
     - Compute hireability AUC
   - Function `test_career_changers(model, test_df)`:
     - CRITICAL per RESEARCH.md: Explicitly test on career changers
     - Detect one-step lag problem (model predicting current role)
     - Return separate metrics for career changers
   - Function `generate_report(metrics)`:
     - Markdown report with all metrics
     - Save to models/evaluation_report.md
   - CLI: `python evaluate.py --checkpoint models/best.pt --data-path data/ --output models/evaluation_report.md`
  </action>
  <verify>
    - `python3 scripts/ml-training/trajectory/export_onnx.py --help` (should show CLI options)
    - `python3 scripts/ml-training/trajectory/calibrate.py --help` (should show CLI options)
    - `python3 scripts/ml-training/trajectory/evaluate.py --help` (should show CLI options)
  </verify>
  <done>ONNX export with dynamic shapes, calibrator training, and evaluation metrics implemented</done>
</task>

</tasks>

<verification>
1. All Python modules import without errors
2. TitleEncoder can encode/decode titles correctly
3. CareerTrajectoryLSTM instantiates with correct architecture
4. train.py CLI shows correct options
5. export_onnx.py uses dynamic_shapes (verify with grep)
6. calibrate.py uses IsotonicRegression (verify with grep)
7. evaluate.py tests career changers explicitly (verify with grep)
</verification>

<success_criteria>
- `scripts/ml-training/trajectory/` directory contains all 7 Python modules
- Requirements.txt includes torch>=2.5.0 and onnxruntime dependencies
- LSTM model uses bidirectional architecture with hidden_dim=16
- Training uses time-aware splits (temporal_split function exists)
- ONNX export uses dynamo=True and dynamic_shapes
- Calibrator uses IsotonicRegression
- Evaluate module tests career changers explicitly
- All CLI scripts show --help output
</success_criteria>

<output>
After completion, create `.planning/phases/13-ml-trajectory-prediction/13-02-SUMMARY.md`
</output>
