---
phase: 13-ml-trajectory-prediction
plan: 03
type: execute
wave: 2
depends_on: ["13-01", "13-02"]
files_modified:
  - services/hh-trajectory-svc/src/inference/onnx-session.ts
  - services/hh-trajectory-svc/src/inference/trajectory-predictor.ts
  - services/hh-trajectory-svc/src/inference/input-encoder.ts
  - services/hh-trajectory-svc/src/inference/calibrator.ts
  - services/hh-trajectory-svc/src/inference/index.ts
  - services/hh-trajectory-svc/src/routes/predict.ts
autonomous: true

must_haves:
  truths:
    - "ONNX model loads via singleton pattern without repeated loading"
    - "Inference returns next role, tenure, and hireability predictions"
    - "Confidence scores are calibrated before returning to client"
    - "Low confidence predictions include uncertainty reason"
  artifacts:
    - path: "services/hh-trajectory-svc/src/inference/onnx-session.ts"
      provides: "ONNX session singleton"
      contains: "InferenceSession"
    - path: "services/hh-trajectory-svc/src/inference/trajectory-predictor.ts"
      provides: "Prediction orchestration"
      exports: ["TrajectoryPredictor"]
    - path: "services/hh-trajectory-svc/src/inference/calibrator.ts"
      provides: "Confidence calibration"
      contains: "calibrate"
  key_links:
    - from: "services/hh-trajectory-svc/src/routes/predict.ts"
      to: "services/hh-trajectory-svc/src/inference/trajectory-predictor.ts"
      via: "import"
      pattern: "from.*inference.*TrajectoryPredictor"
    - from: "services/hh-trajectory-svc/src/inference/trajectory-predictor.ts"
      to: "services/hh-trajectory-svc/src/inference/onnx-session.ts"
      via: "import"
      pattern: "from.*onnx-session"
---

<objective>
Implement ONNX inference engine for trajectory predictions in hh-trajectory-svc

Purpose: Wire up the trained ONNX model to the Fastify service for real-time inference. Implements singleton session management, input encoding, confidence calibration, and uncertainty explanation.

Output: Working /predict endpoint returning ML-based trajectory predictions with calibrated confidence.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-ml-trajectory-prediction/13-RESEARCH.md
@.planning/phases/13-ml-trajectory-prediction/13-01-SUMMARY.md
@.planning/phases/13-ml-trajectory-prediction/13-02-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create ONNX session singleton and input encoder</name>
  <files>
    services/hh-trajectory-svc/src/inference/onnx-session.ts
    services/hh-trajectory-svc/src/inference/input-encoder.ts
    services/hh-trajectory-svc/src/inference/index.ts
  </files>
  <action>
Create ONNX inference infrastructure:

1. Create `services/hh-trajectory-svc/src/inference/onnx-session.ts`:
   - Class `ONNXSession`:
     - Private static `instance: ort.InferenceSession | null`
     - Private static `initPromise: Promise<ort.InferenceSession> | null`
     - Static async `getInstance(modelPath: string): Promise<ort.InferenceSession>`:
       - Return cached instance if exists
       - Create via `ort.InferenceSession.create()` with options:
         - executionProviders: ['cpu']
         - graphOptimizationLevel: 'all'
         - enableCpuMemArena: true
         - enableMemPattern: true
         - intraOpNumThreads: 4
       - Cache promise to prevent duplicate initialization
     - Static `isInitialized(): boolean`
     - Static async `dispose()`: Cleanup for graceful shutdown
   - Export default ONNXSession class

2. Create `services/hh-trajectory-svc/src/inference/input-encoder.ts`:
   - Class `InputEncoder`:
     - Private `titleVocab: Map<string, number>`
     - Private `indexToTitle: Map<number, string>`
     - Constructor: Load vocabulary from JSON file
     - Method `encode(titles: string[]): BigInt64Array`:
       - Normalize: lowercase, trim
       - Map titles to indices (0 for unknown)
       - Return BigInt64Array for ONNX tensor
     - Method `decode(index: number): string`:
       - Return title string from index
     - Method `getVocabSize(): number`
   - Handle common variations (Sr. -> Senior, Jr. -> Junior, etc.)
   - Export default InputEncoder class

3. Create `services/hh-trajectory-svc/src/inference/index.ts`:
   - Export all inference modules
   - Barrel export for clean imports
  </action>
  <verify>
    - `npx tsc --noEmit -p services/hh-trajectory-svc/tsconfig.json`
    - Check imports: `grep -r "onnxruntime-node" services/hh-trajectory-svc/src/`
  </verify>
  <done>ONNX session singleton and input encoder implemented with proper typing</done>
</task>

<task type="auto">
  <name>Task 2: Create calibrator and trajectory predictor</name>
  <files>
    services/hh-trajectory-svc/src/inference/calibrator.ts
    services/hh-trajectory-svc/src/inference/trajectory-predictor.ts
  </files>
  <action>
Create prediction orchestration:

1. Create `services/hh-trajectory-svc/src/inference/calibrator.ts`:
   - Interface `CalibrationData`:
     - `breakpoints: number[]` (input confidence values)
     - `values: number[]` (calibrated confidence values)
   - Class `Calibrator`:
     - Private `data: CalibrationData | null`
     - Constructor: Load calibration data from JSON file
     - Method `calibrate(rawConfidence: number): number`:
       - Linear interpolation between breakpoints
       - Return calibrated confidence
       - Handle edge cases (below min, above max)
     - Method `isLowConfidence(calibrated: number): boolean`:
       - Return true if < 0.6 (threshold from config)
   - Export default Calibrator class

2. Create `services/hh-trajectory-svc/src/inference/trajectory-predictor.ts`:
   - Class `TrajectoryPredictor`:
     - Private `session: ort.InferenceSession | null`
     - Private `encoder: InputEncoder`
     - Private `calibrator: Calibrator`
     - Private `config: Config`
     - Async method `initialize()`:
       - Load ONNX model via ONNXSession.getInstance()
       - Initialize encoder and calibrator
     - Async method `predict(request: PredictRequest): Promise<TrajectoryPrediction>`:
       - Encode title sequence via InputEncoder
       - Create tensors: title_ids (BigInt64Array), lengths (BigInt64Array)
       - Run inference: `session.run({ title_ids, lengths })`
       - Extract outputs: next_role_logits, tenure_pred, hireability
       - Apply softmax to logits for probabilities
       - Get top prediction and raw confidence
       - Calibrate confidence via Calibrator
       - Determine if low confidence
       - Generate uncertainty reason if low confidence:
         - "Limited career history data" if sequence < 3
         - "Unusual career pattern" if entropy high
         - "Ambiguous next role" if top-2 confidence gap < 0.1
       - Return TrajectoryPrediction with all fields
     - Method `isInitialized(): boolean`
   - Export default TrajectoryPredictor class
  </action>
  <verify>
    - `npx tsc --noEmit -p services/hh-trajectory-svc/tsconfig.json`
    - Check calibrator: `grep -n "calibrate" services/hh-trajectory-svc/src/inference/calibrator.ts`
    - Check predictor: `grep -n "uncertaintyReason" services/hh-trajectory-svc/src/inference/trajectory-predictor.ts`
  </verify>
  <done>Calibrator and TrajectoryPredictor implemented with uncertainty explanation</done>
</task>

<task type="auto">
  <name>Task 3: Wire predict route to use TrajectoryPredictor</name>
  <files>
    services/hh-trajectory-svc/src/routes/predict.ts
    services/hh-trajectory-svc/src/index.ts
  </files>
  <action>
Connect inference to HTTP routes:

1. Update `services/hh-trajectory-svc/src/routes/predict.ts`:
   - Import TrajectoryPredictor from inference module
   - Create singleton predictor instance at module level
   - POST `/predict` handler:
     - Validate request body against PredictRequest schema
     - Check if predictor is initialized, return 503 if not
     - Call predictor.predict(request)
     - Return PredictResponse with prediction
     - Handle errors gracefully (return 500 with error message)
   - GET `/predict/health` handler:
     - Return predictor initialization status
     - Return model path and config info

2. Update `services/hh-trajectory-svc/src/index.ts`:
   - Import TrajectoryPredictor
   - Initialize predictor in background after server starts (lazy init)
   - Use setImmediate() to not block startup
   - Log initialization progress
   - Add predictor status to /health response
   - Handle graceful shutdown: dispose ONNX session

Ensure sub-50ms inference target per RESEARCH.md:
- Log inference duration for monitoring
- Use efficient tensor creation
- Avoid unnecessary allocations in hot path
  </action>
  <verify>
    - `npm run build --workspace=@hh/hh-trajectory-svc`
    - `npm run typecheck --workspace=@hh/hh-trajectory-svc`
    - If test model exists: start service and test `/predict` endpoint
  </verify>
  <done>Predict route uses TrajectoryPredictor for ML inference with calibrated confidence</done>
</task>

</tasks>

<verification>
1. TypeScript compiles: `npm run typecheck --workspace=@hh/hh-trajectory-svc`
2. Service builds: `npm run build --workspace=@hh/hh-trajectory-svc`
3. ONNX session uses singleton pattern (grep for "static instance")
4. Calibrator implements interpolation (grep for "interpolate" or breakpoints logic)
5. TrajectoryPredictor returns uncertainty reasons for low confidence
6. Predict route is async and handles errors
7. Inference logging includes duration for latency monitoring
</verification>

<success_criteria>
- ONNX session singleton prevents repeated model loading
- Input encoder handles title normalization and unknown tokens
- Calibrator applies isotonic regression via interpolation
- TrajectoryPredictor returns all prediction fields:
  - nextRole with calibrated confidence
  - tenureMonths.min and tenureMonths.max
  - hireability score (0-1)
  - lowConfidence boolean
  - uncertaintyReason when applicable
- /predict endpoint returns TrajectoryPrediction
- TypeScript compiles without errors
</success_criteria>

<output>
After completion, create `.planning/phases/13-ml-trajectory-prediction/13-03-SUMMARY.md`
</output>
