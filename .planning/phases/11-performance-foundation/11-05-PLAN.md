---
phase: 11-performance-foundation
plan: 05
type: execute
wave: 3
depends_on: ["11-01", "11-03", "11-04"]
files_modified:
  - services/hh-search-svc/src/performance-tracker.ts
  - services/hh-search-svc/src/routes/search.ts
  - scripts/embedding-backfill-worker.ts
  - scripts/run-embedding-backfill.sh
autonomous: true
user_setup: []

must_haves:
  truths:
    - "Performance tracker records per-stage latencies with index type"
    - "Response headers include Server-Timing for observability"
    - "Embedding backfill worker can process all 23K+ candidates"
    - "p95 latency can be measured and alerted on"
  artifacts:
    - path: "services/hh-search-svc/src/performance-tracker.ts"
      provides: "Extended performance tracking with Stage latencies"
      contains: "indexType"
    - path: "scripts/embedding-backfill-worker.ts"
      provides: "Batch embedding pre-computation worker"
      contains: "batchSize"
    - path: "scripts/run-embedding-backfill.sh"
      provides: "Script to run embedding backfill"
      contains: "ts-node"
  key_links:
    - from: "services/hh-search-svc/src/routes/search.ts"
      to: "services/hh-search-svc/src/performance-tracker.ts"
      via: "Server-Timing header"
      pattern: "Server-Timing"
---

<objective>
Implement performance tracking, embedding pre-computation worker, and observability for p95 latency measurement.

Purpose: PERF-01 requires p95 latency under 500ms. This plan adds the instrumentation to measure and verify the target, plus embedding pre-computation to eliminate on-demand embedding generation latency.
Output: Enhanced performance tracker, Server-Timing headers, embedding backfill worker.
</objective>

<execution_context>
@/Users/delimatsuo/.claude/get-shit-done/workflows/execute-plan.md
@/Users/delimatsuo/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/11-performance-foundation/11-RESEARCH.md
@services/hh-search-svc/src/performance-tracker.ts
@services/hh-search-svc/src/search-service.ts
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extend performance tracker with stage latencies</name>
  <files>services/hh-search-svc/src/performance-tracker.ts</files>
  <action>
    Extend PerformanceSample and PerformanceTracker to capture Phase 11 metrics:

    1. Update PerformanceSample interface:
    ```typescript
    export interface PerformanceSample {
      totalMs: number;
      embeddingMs?: number;
      retrievalMs?: number;
      rerankMs?: number;
      cacheHit: boolean;
      rerankApplied?: boolean;
      timestamp?: number;

      // Phase 11: Extended metrics
      indexType?: 'hnsw' | 'diskann';
      vectorSearchMs?: number;
      textSearchMs?: number;
      scoringMs?: number;
      parallelSavingsMs?: number;
      poolWaitMs?: number;

      // Cache metrics
      embeddingCacheHit?: boolean;
      rerankCacheHit?: boolean;
      specialtyCacheHit?: boolean;
    }
    ```

    2. Add percentile calculation method:
    ```typescript
    /**
     * Calculate percentile from recorded samples.
     * @param percentile - 0 to 100 (e.g., 95 for p95)
     */
    getPercentile(percentile: number): number {
      if (this.samples.length === 0) {
        return 0;
      }

      const sorted = [...this.samples].map(s => s.totalMs).sort((a, b) => a - b);
      const index = Math.ceil((percentile / 100) * sorted.length) - 1;
      return sorted[Math.max(0, index)];
    }

    /**
     * Get p50, p95, p99 latencies.
     */
    getLatencyPercentiles(): { p50: number; p95: number; p99: number } {
      return {
        p50: this.getPercentile(50),
        p95: this.getPercentile(95),
        p99: this.getPercentile(99)
      };
    }
    ```

    3. Add stage breakdown aggregation:
    ```typescript
    /**
     * Get average latency breakdown by stage.
     */
    getStageBreakdown(): {
      embedding: number;
      vectorSearch: number;
      textSearch: number;
      scoring: number;
      rerank: number;
      total: number;
    } {
      if (this.samples.length === 0) {
        return { embedding: 0, vectorSearch: 0, textSearch: 0, scoring: 0, rerank: 0, total: 0 };
      }

      const sum = (arr: (number | undefined)[]): number =>
        arr.reduce((a, b) => (a ?? 0) + (b ?? 0), 0) ?? 0;
      const n = this.samples.length;

      return {
        embedding: sum(this.samples.map(s => s.embeddingMs)) / n,
        vectorSearch: sum(this.samples.map(s => s.vectorSearchMs)) / n,
        textSearch: sum(this.samples.map(s => s.textSearchMs)) / n,
        scoring: sum(this.samples.map(s => s.scoringMs)) / n,
        rerank: sum(this.samples.map(s => s.rerankMs)) / n,
        total: sum(this.samples.map(s => s.totalMs)) / n
      };
    }
    ```

    4. Add index type breakdown:
    ```typescript
    /**
     * Get latency breakdown by index type.
     */
    getLatencyByIndexType(): { hnsw: number; diskann: number } {
      const hnsw = this.samples.filter(s => s.indexType === 'hnsw');
      const diskann = this.samples.filter(s => s.indexType === 'diskann');

      const avgMs = (arr: PerformanceSample[]): number =>
        arr.length > 0 ? arr.reduce((a, b) => a + b.totalMs, 0) / arr.length : 0;

      return {
        hnsw: avgMs(hnsw),
        diskann: avgMs(diskann)
      };
    }
    ```
  </action>
  <verify>
    Run: `npm run build --prefix services/hh-search-svc`
    Run: `npm test --prefix services/hh-search-svc -- --grep "performance"`
    Expected: Build succeeds, performance tracker tests pass
  </verify>
  <done>Performance tracker calculates p50/p95/p99 and stage breakdowns</done>
</task>

<task type="auto">
  <name>Task 2: Add Server-Timing headers for observability</name>
  <files>services/hh-search-svc/src/routes/search.ts</files>
  <action>
    Add Server-Timing response headers for browser DevTools and observability tools:

    1. After search completes, add Server-Timing header to response:
    ```typescript
    // Build Server-Timing header for observability
    // Format: <name>;dur=<duration>;desc="<description>"
    const serverTimingParts: string[] = [];

    if (response.timings.embeddingMs !== undefined) {
      serverTimingParts.push(`embedding;dur=${response.timings.embeddingMs};desc="Embedding generation"`);
    }
    if (response.timings.retrievalMs !== undefined) {
      serverTimingParts.push(`retrieval;dur=${response.timings.retrievalMs};desc="Vector+Text retrieval"`);
    }
    if (response.timings.rankingMs !== undefined) {
      serverTimingParts.push(`scoring;dur=${response.timings.rankingMs};desc="Signal scoring"`);
    }
    if (response.timings.rerankMs !== undefined) {
      serverTimingParts.push(`rerank;dur=${response.timings.rerankMs};desc="LLM reranking"`);
    }
    serverTimingParts.push(`total;dur=${response.timings.totalMs};desc="Total search time"`);

    // Add cache status
    serverTimingParts.push(`cache;desc="${response.cacheHit ? 'hit' : 'miss'}"`);

    // Set the header
    reply.header('Server-Timing', serverTimingParts.join(', '));
    ```

    2. Add X-Response-Time header for simple latency tracking:
    ```typescript
    reply.header('X-Response-Time', `${response.timings.totalMs}ms`);
    ```

    3. Add cache hit rate header for monitoring:
    ```typescript
    if (response.metadata?.rerank?.cacheHit !== undefined) {
      reply.header('X-Rerank-Cache', response.metadata.rerank.cacheHit ? 'hit' : 'miss');
    }
    ```

    4. Log p95 warning if latency exceeds target:
    ```typescript
    const p95Target = 500; // 500ms target
    if (response.timings.totalMs > p95Target) {
      fastify.log.warn({
        requestId: request.headers['x-request-id'],
        totalMs: response.timings.totalMs,
        p95Target,
        timings: response.timings
      }, 'Search latency exceeded p95 target');
    }
    ```
  </action>
  <verify>
    Run: `npm run build --prefix services/hh-search-svc`
    Run: `curl -v localhost:7102/search -d '{"query":"test"}' | grep -i "server-timing"`
    Expected: Build succeeds, Server-Timing header present in response
  </verify>
  <done>Server-Timing headers provide observability for latency breakdown</done>
</task>

<task type="auto">
  <name>Task 3: Create embedding backfill worker</name>
  <files>scripts/embedding-backfill-worker.ts, scripts/run-embedding-backfill.sh</files>
  <action>
    Create a batch worker to pre-compute embeddings for all candidates (PERF-04):

    1. Create scripts/embedding-backfill-worker.ts:
    ```typescript
    /**
     * Embedding Backfill Worker
     *
     * Pre-computes embeddings for all candidates in the database.
     * This eliminates on-demand embedding generation during search.
     *
     * Usage: npx ts-node scripts/embedding-backfill-worker.ts [--batch-size=100] [--concurrency=10]
     */

    import { Pool } from 'pg';
    import { GoogleGenerativeAI } from '@google/generative-ai';

    interface Config {
      batchSize: number;
      concurrency: number;
      dryRun: boolean;
    }

    interface CandidateRow {
      candidate_id: string;
      tenant_id: string;
      full_name: string | null;
      current_title: string | null;
      headline: string | null;
      skills: string[] | null;
    }

    async function generateEmbedding(
      genai: GoogleGenerativeAI,
      text: string
    ): Promise<number[]> {
      const model = genai.getGenerativeModel({ model: 'text-embedding-004' });
      const result = await model.embedContent(text);
      return result.embedding.values;
    }

    function buildEmbeddingText(candidate: CandidateRow): string {
      const parts: string[] = [];
      if (candidate.current_title) parts.push(candidate.current_title);
      if (candidate.headline) parts.push(candidate.headline);
      if (candidate.skills?.length) parts.push(candidate.skills.join(', '));
      return parts.join(' | ') || 'No details available';
    }

    async function processBatch(
      pool: Pool,
      genai: GoogleGenerativeAI,
      candidates: CandidateRow[],
      config: Config
    ): Promise<{ processed: number; errors: number }> {
      let processed = 0;
      let errors = 0;

      // Process with concurrency limit
      const chunks: CandidateRow[][] = [];
      for (let i = 0; i < candidates.length; i += config.concurrency) {
        chunks.push(candidates.slice(i, i + config.concurrency));
      }

      for (const chunk of chunks) {
        const results = await Promise.allSettled(
          chunk.map(async (candidate) => {
            const text = buildEmbeddingText(candidate);
            const embedding = await generateEmbedding(genai, text);

            if (!config.dryRun) {
              await pool.query(
                `INSERT INTO search.candidate_embeddings
                  (tenant_id, entity_id, embedding, embedding_text, model_version, chunk_type, updated_at)
                VALUES ($1, $2, $3, $4, $5, $6, NOW())
                ON CONFLICT (tenant_id, entity_id, chunk_type)
                DO UPDATE SET
                  embedding = EXCLUDED.embedding,
                  embedding_text = EXCLUDED.embedding_text,
                  model_version = EXCLUDED.model_version,
                  updated_at = NOW()`,
                [candidate.tenant_id, candidate.candidate_id, JSON.stringify(embedding), text, 'text-embedding-004', 'default']
              );
            }

            return candidate.candidate_id;
          })
        );

        for (const result of results) {
          if (result.status === 'fulfilled') {
            processed++;
          } else {
            errors++;
            console.error('Error processing candidate:', result.reason);
          }
        }

        // Rate limiting: 60 requests per minute for Gemini API
        await new Promise(resolve => setTimeout(resolve, config.concurrency * 100));
      }

      return { processed, errors };
    }

    async function main() {
      // Parse arguments
      const args = process.argv.slice(2);
      const config: Config = {
        batchSize: parseInt(args.find(a => a.startsWith('--batch-size='))?.split('=')[1] ?? '100'),
        concurrency: parseInt(args.find(a => a.startsWith('--concurrency='))?.split('=')[1] ?? '10'),
        dryRun: args.includes('--dry-run')
      };

      console.log('Embedding Backfill Worker starting with config:', config);

      // Initialize clients
      const pool = new Pool({
        host: process.env.PGVECTOR_HOST ?? '127.0.0.1',
        port: parseInt(process.env.PGVECTOR_PORT ?? '5432'),
        database: process.env.PGVECTOR_DATABASE ?? 'headhunter',
        user: process.env.PGVECTOR_USER ?? 'postgres',
        password: process.env.PGVECTOR_PASSWORD ?? ''
      });

      const genai = new GoogleGenerativeAI(process.env.GEMINI_API_KEY ?? '');

      try {
        // Count total candidates
        const countResult = await pool.query(
          'SELECT COUNT(*) as total FROM search.candidate_profiles'
        );
        const total = parseInt(countResult.rows[0].total);
        console.log(`Total candidates to process: ${total}`);

        // Process in batches
        let offset = 0;
        let totalProcessed = 0;
        let totalErrors = 0;

        while (offset < total) {
          const batchResult = await pool.query<CandidateRow>(
            `SELECT candidate_id, tenant_id, full_name, current_title, headline, skills
            FROM search.candidate_profiles
            ORDER BY candidate_id
            LIMIT $1 OFFSET $2`,
            [config.batchSize, offset]
          );

          if (batchResult.rows.length === 0) break;

          console.log(`Processing batch ${offset / config.batchSize + 1} (${offset}-${offset + batchResult.rows.length})...`);

          const { processed, errors } = await processBatch(pool, genai, batchResult.rows, config);
          totalProcessed += processed;
          totalErrors += errors;

          console.log(`Batch complete. Total progress: ${totalProcessed}/${total} (${errors} errors)`);

          offset += config.batchSize;
        }

        console.log(`\nBackfill complete!`);
        console.log(`Processed: ${totalProcessed}`);
        console.log(`Errors: ${totalErrors}`);
        console.log(`Success rate: ${((totalProcessed / (totalProcessed + totalErrors)) * 100).toFixed(1)}%`);

      } finally {
        await pool.end();
      }
    }

    main().catch(console.error);
    ```

    2. Create scripts/run-embedding-backfill.sh:
    ```bash
    #!/bin/bash
    # Run embedding backfill worker
    # Usage: ./scripts/run-embedding-backfill.sh [--dry-run]

    set -e

    SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
    PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"

    # Load environment if .env exists
    if [ -f "$PROJECT_ROOT/.env" ]; then
      export $(cat "$PROJECT_ROOT/.env" | grep -v '^#' | xargs)
    fi

    echo "Starting embedding backfill worker..."
    echo "Database: ${PGVECTOR_HOST:-127.0.0.1}:${PGVECTOR_PORT:-5432}/${PGVECTOR_DATABASE:-headhunter}"

    cd "$PROJECT_ROOT"
    npx ts-node scripts/embedding-backfill-worker.ts "$@"
    ```

    3. Make the shell script executable:
    ```bash
    chmod +x scripts/run-embedding-backfill.sh
    ```
  </action>
  <verify>
    Run: `npx tsc --noEmit scripts/embedding-backfill-worker.ts`
    Run: `./scripts/run-embedding-backfill.sh --dry-run --batch-size=10`
    Expected: TypeScript compiles, dry run completes without database changes
  </verify>
  <done>Embedding backfill worker can pre-compute embeddings for all 23K+ candidates</done>
</task>

</tasks>

<verification>
1. TypeScript compiles without errors
2. Existing tests continue to pass
3. Server-Timing headers appear in search responses
4. Performance tracker provides p50/p95/p99 percentiles
5. Embedding backfill worker runs in dry-run mode
6. Latency warnings logged when exceeding target
</verification>

<success_criteria>
- Performance tracker calculates p50/p95/p99 latencies
- Server-Timing headers provide browser DevTools observability
- X-Response-Time header shows total latency
- Embedding backfill worker created and testable
- Warning logs when latency exceeds 500ms target
- Stage breakdown by index type available
</success_criteria>

<output>
After completion, create `.planning/phases/11-performance-foundation/11-05-SUMMARY.md`
</output>
