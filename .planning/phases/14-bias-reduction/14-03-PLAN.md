---
phase: 14-bias-reduction
plan: 03
type: execute
wave: 2
depends_on: ["14-01"]
files_modified:
  - scripts/bias_metrics_worker.py
  - scripts/requirements.txt
  - scripts/migrations/011_bias_tables.sql
  - services/hh-search-svc/src/types.ts
  - services/hh-search-svc/src/bias/selection-events.ts
  - services/hh-search-svc/src/bias/selection-events.test.ts
autonomous: true

must_haves:
  truths:
    - "Fairlearn worker computes selection rates by dimension"
    - "Impact ratio (four-fifths rule) calculated correctly"
    - "Selection events logged for bias tracking"
    - "Statistical significance testing for small samples"
  artifacts:
    - path: "scripts/bias_metrics_worker.py"
      provides: "Fairlearn-based bias metrics computation"
      min_lines: 150
    - path: "services/hh-search-svc/src/bias/selection-events.ts"
      provides: "Selection event logging for bias tracking"
      exports: ["logSelectionEvent", "SelectionEvent"]
  key_links:
    - from: "scripts/bias_metrics_worker.py"
      to: "PostgreSQL"
      via: "psycopg2 connection"
      pattern: "psycopg2.connect"
---

<objective>
Create Fairlearn-based bias metrics worker and selection event logging for BIAS-03/BIAS-04 requirements.

Purpose: Enable computation of selection rates and impact ratios (four-fifths rule) to detect potential adverse impact in candidate selection.

Output: Python worker for metrics computation, TypeScript module for event logging.
</objective>

<execution_context>
@~/.claude/get-shit-done/workflows/execute-plan.md
@~/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/14-bias-reduction/14-RESEARCH.md

Key code files:
@services/hh-search-svc/src/bias/types.ts (from Plan 01)
@scripts/requirements.txt
</context>

<tasks>

<task type="auto">
  <name>Task 0: Create database migration for bias tables</name>
  <files>scripts/migrations/011_bias_tables.sql</files>
  <action>
Create `scripts/migrations/011_bias_tables.sql` with the schema for selection events and bias metrics:

```sql
-- Migration 011: Create tables for bias tracking (Phase 14)
-- These tables support BIAS-03 (selection event logging) and BIAS-04 (bias metrics computation)

-- Selection events table for tracking candidate interactions
CREATE TABLE IF NOT EXISTS selection_events (
    event_id VARCHAR(64) PRIMARY KEY,
    timestamp TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    candidate_id VARCHAR(255) NOT NULL,
    event_type VARCHAR(50) NOT NULL CHECK (event_type IN ('shown', 'clicked', 'shortlisted', 'contacted', 'interviewed', 'hired')),
    search_id VARCHAR(255) NOT NULL,
    tenant_id VARCHAR(255) NOT NULL,
    user_id_hash VARCHAR(64) NOT NULL,

    -- Inferred dimensions for bias analysis (no actual demographics collected)
    company_tier VARCHAR(50) CHECK (company_tier IN ('faang', 'enterprise', 'startup', 'other')),
    experience_band VARCHAR(20) CHECK (experience_band IN ('0-3', '3-7', '7-15', '15+')),
    specialty VARCHAR(50) CHECK (specialty IN ('backend', 'frontend', 'fullstack', 'devops', 'data', 'ml', 'mobile', 'other')),

    -- Search context
    rank INTEGER,
    score NUMERIC(5, 4),

    -- Indexes
    created_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW()
);

-- Indexes for common query patterns
CREATE INDEX IF NOT EXISTS idx_selection_events_tenant_timestamp
    ON selection_events (tenant_id, timestamp);
CREATE INDEX IF NOT EXISTS idx_selection_events_search_id
    ON selection_events (search_id);
CREATE INDEX IF NOT EXISTS idx_selection_events_event_type
    ON selection_events (event_type);
CREATE INDEX IF NOT EXISTS idx_selection_events_dimensions
    ON selection_events (tenant_id, company_tier, experience_band, specialty);

-- Bias metrics table for storing computed results
CREATE TABLE IF NOT EXISTS bias_metrics (
    id SERIAL PRIMARY KEY,
    tenant_id VARCHAR(255) NOT NULL,
    computed_at TIMESTAMP WITH TIME ZONE NOT NULL DEFAULT NOW(),
    metrics_json JSONB NOT NULL,

    -- For querying latest metrics
    CONSTRAINT unique_tenant_computed UNIQUE (tenant_id, computed_at)
);

CREATE INDEX IF NOT EXISTS idx_bias_metrics_tenant_computed
    ON bias_metrics (tenant_id, computed_at DESC);

-- Comment for documentation
COMMENT ON TABLE selection_events IS 'Tracks candidate selection events for bias analysis (BIAS-03)';
COMMENT ON TABLE bias_metrics IS 'Stores computed bias metrics from Fairlearn worker (BIAS-04)';
```

Run the migration:
```bash
# Local development
psql -h localhost -U headhunter -d headhunter -f scripts/migrations/011_bias_tables.sql

# Or via environment variables
PGPASSWORD=$POSTGRES_PASSWORD psql -h $POSTGRES_HOST -U $POSTGRES_USER -d $POSTGRES_DB -f scripts/migrations/011_bias_tables.sql
```
  </action>
  <verify>
    psql -h localhost -U headhunter -d headhunter -c "\\dt selection_events" 2>/dev/null || echo "Will be created during execution"
  </verify>
  <done>
    Migration file created for selection_events and bias_metrics tables with appropriate indexes.
  </done>
</task>

<task type="auto">
  <name>Task 1: Add Fairlearn to requirements.txt</name>
  <files>scripts/requirements.txt</files>
  <action>
Update `scripts/requirements.txt` to add Fairlearn dependency:

Add the following lines after the existing data processing dependencies:

```
# Bias metrics (Phase 14)
fairlearn>=0.13.0
scipy>=1.11.0
```

The `scipy` addition is for Fisher's exact test on small samples.
  </action>
  <verify>
    grep "fairlearn" scripts/requirements.txt
  </verify>
  <done>
    Fairlearn 0.13.0+ and scipy added to requirements.txt
  </done>
</task>

<task type="auto">
  <name>Task 2: Create selection event logging module</name>
  <files>
    services/hh-search-svc/src/bias/selection-events.ts
    services/hh-search-svc/src/bias/selection-events.test.ts
    services/hh-search-svc/src/bias/index.ts
  </files>
  <action>
Create `services/hh-search-svc/src/bias/selection-events.ts`:

```typescript
/**
 * Selection Event Logging for BIAS-03/BIAS-04
 *
 * Logs candidate selection events (shown, clicked, shortlisted, hired)
 * to enable bias metrics computation using Fairlearn.
 *
 * Events are logged to PostgreSQL for aggregation by the bias_metrics_worker.
 */

import { Pool } from 'pg';
import { getLogger } from '@hh/common';

const logger = getLogger({ module: 'selection-events' });

/**
 * Types of selection events to track.
 * Multiple thresholds enable granular bias analysis.
 */
export type SelectionEventType =
  | 'shown'         // Candidate appeared in search results
  | 'clicked'       // Recruiter clicked to view details
  | 'shortlisted'   // Added to shortlist
  | 'contacted'     // Reached out to candidate
  | 'interviewed'   // Scheduled/completed interview
  | 'hired';        // Made offer/hired

/**
 * Inferred demographic dimensions for bias analysis.
 * We don't collect actual demographics - these are proxy dimensions
 * that can be analyzed without collecting protected information.
 */
export interface InferredDimensions {
  /** Company tier based on prior employers */
  companyTier: 'faang' | 'enterprise' | 'startup' | 'other';

  /** Experience band */
  experienceBand: '0-3' | '3-7' | '7-15' | '15+';

  /** Technical specialty */
  specialty: 'backend' | 'frontend' | 'fullstack' | 'devops' | 'data' | 'ml' | 'mobile' | 'other';
}

/**
 * Selection event to be logged for bias tracking.
 */
export interface SelectionEvent {
  /** Unique event ID */
  eventId: string;

  /** Timestamp of event */
  timestamp: Date;

  /** Candidate ID (for aggregation, not PII) */
  candidateId: string;

  /** Type of selection event */
  eventType: SelectionEventType;

  /** Search/session ID (groups events from same search) */
  searchId: string;

  /** Tenant ID for multi-tenant isolation */
  tenantId: string;

  /** User who performed the action (hashed for privacy) */
  userIdHash: string;

  /** Inferred dimensions for bias grouping */
  dimensions: InferredDimensions;

  /** Candidate's rank in results (for 'shown' events) */
  rank?: number;

  /** Match score at time of event */
  score?: number;
}

/**
 * Infer company tier from company names.
 * Maps to bias analysis dimensions.
 */
export function inferCompanyTier(companies: string[]): InferredDimensions['companyTier'] {
  const faangLike = [
    'google', 'meta', 'facebook', 'amazon', 'apple', 'microsoft', 'netflix',
    'uber', 'airbnb', 'stripe', 'linkedin', 'twitter', 'salesforce', 'oracle',
    'snap', 'spotify', 'doordash', 'instacart', 'coinbase', 'robinhood'
  ];

  const enterprise = [
    'ibm', 'cisco', 'intel', 'dell', 'hp', 'vmware', 'sap', 'adobe',
    'workday', 'servicenow', 'splunk', 'atlassian', 'twilio', 'datadog',
    'snowflake', 'palantir', 'databricks'
  ];

  const normalized = companies.map(c => c.toLowerCase().replace(/[^a-z]/g, ''));

  for (const company of normalized) {
    if (faangLike.some(f => company.includes(f))) return 'faang';
  }

  for (const company of normalized) {
    if (enterprise.some(e => company.includes(e))) return 'enterprise';
  }

  // Check for startup indicators
  const hasStartupIndicators = companies.some(c =>
    c.toLowerCase().includes('startup') ||
    c.toLowerCase().includes('seed') ||
    c.toLowerCase().includes('series')
  );
  if (hasStartupIndicators) return 'startup';

  return 'other';
}

/**
 * Infer experience band from years of experience.
 */
export function inferExperienceBand(yearsExperience: number | undefined): InferredDimensions['experienceBand'] {
  if (yearsExperience === undefined) return '3-7'; // Default to mid-range
  if (yearsExperience < 3) return '0-3';
  if (yearsExperience < 7) return '3-7';
  if (yearsExperience < 15) return '7-15';
  return '15+';
}

/**
 * Infer specialty from skills and title.
 */
export function inferSpecialty(skills: string[], title?: string): InferredDimensions['specialty'] {
  const normalized = skills.map(s => s.toLowerCase());
  const titleLower = (title || '').toLowerCase();

  // Check title first for strong signals
  if (titleLower.includes('frontend') || titleLower.includes('front-end')) return 'frontend';
  if (titleLower.includes('backend') || titleLower.includes('back-end')) return 'backend';
  if (titleLower.includes('fullstack') || titleLower.includes('full-stack')) return 'fullstack';
  if (titleLower.includes('devops') || titleLower.includes('sre') || titleLower.includes('platform')) return 'devops';
  if (titleLower.includes('data engineer') || titleLower.includes('data scientist')) return 'data';
  if (titleLower.includes('ml') || titleLower.includes('machine learning') || titleLower.includes('ai')) return 'ml';
  if (titleLower.includes('mobile') || titleLower.includes('ios') || titleLower.includes('android')) return 'mobile';

  // Fall back to skill analysis
  const frontendSkills = ['react', 'vue', 'angular', 'css', 'html', 'javascript', 'typescript', 'nextjs'];
  const backendSkills = ['java', 'python', 'go', 'rust', 'c++', 'nodejs', 'postgresql', 'mongodb'];
  const devopsSkills = ['kubernetes', 'docker', 'terraform', 'aws', 'gcp', 'azure', 'jenkins', 'ci/cd'];
  const dataSkills = ['sql', 'spark', 'hadoop', 'airflow', 'dbt', 'snowflake', 'bigquery'];
  const mlSkills = ['pytorch', 'tensorflow', 'scikit-learn', 'pandas', 'numpy', 'ml', 'deep learning'];
  const mobileSkills = ['swift', 'kotlin', 'react native', 'flutter', 'ios', 'android'];

  const counts = {
    frontend: normalized.filter(s => frontendSkills.some(f => s.includes(f))).length,
    backend: normalized.filter(s => backendSkills.some(b => s.includes(b))).length,
    devops: normalized.filter(s => devopsSkills.some(d => s.includes(d))).length,
    data: normalized.filter(s => dataSkills.some(d => s.includes(d))).length,
    ml: normalized.filter(s => mlSkills.some(m => s.includes(m))).length,
    mobile: normalized.filter(s => mobileSkills.some(m => s.includes(m))).length,
  };

  const maxCount = Math.max(...Object.values(counts));
  if (maxCount === 0) return 'other';

  // Check for fullstack (both frontend and backend)
  if (counts.frontend >= 2 && counts.backend >= 2) return 'fullstack';

  const maxKey = Object.entries(counts).find(([, v]) => v === maxCount)?.[0];
  return (maxKey as InferredDimensions['specialty']) || 'other';
}

/**
 * Create a selection event from candidate data.
 */
export function createSelectionEvent(
  eventType: SelectionEventType,
  candidateId: string,
  searchId: string,
  tenantId: string,
  userIdHash: string,
  candidateData: {
    companies?: string[];
    yearsExperience?: number;
    skills?: string[];
    title?: string;
    rank?: number;
    score?: number;
  }
): SelectionEvent {
  return {
    eventId: `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`,
    timestamp: new Date(),
    candidateId,
    eventType,
    searchId,
    tenantId,
    userIdHash,
    dimensions: {
      companyTier: inferCompanyTier(candidateData.companies || []),
      experienceBand: inferExperienceBand(candidateData.yearsExperience),
      specialty: inferSpecialty(candidateData.skills || [], candidateData.title),
    },
    rank: candidateData.rank,
    score: candidateData.score,
  };
}

/**
 * Log selection event to PostgreSQL for bias metrics.
 */
export async function logSelectionEvent(
  pool: Pool,
  event: SelectionEvent
): Promise<void> {
  const query = `
    INSERT INTO selection_events (
      event_id, timestamp, candidate_id, event_type,
      search_id, tenant_id, user_id_hash,
      company_tier, experience_band, specialty,
      rank, score
    ) VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9, $10, $11, $12)
    ON CONFLICT (event_id) DO NOTHING
  `;

  try {
    await pool.query(query, [
      event.eventId,
      event.timestamp,
      event.candidateId,
      event.eventType,
      event.searchId,
      event.tenantId,
      event.userIdHash,
      event.dimensions.companyTier,
      event.dimensions.experienceBand,
      event.dimensions.specialty,
      event.rank,
      event.score,
    ]);

    logger.debug(
      { eventId: event.eventId, eventType: event.eventType },
      'Selection event logged'
    );
  } catch (error) {
    logger.error(
      { error, eventId: event.eventId },
      'Failed to log selection event'
    );
    // Don't throw - bias logging should not break search
  }
}

/**
 * Batch log multiple selection events (for 'shown' events).
 */
export async function logSelectionEventsBatch(
  pool: Pool,
  events: SelectionEvent[]
): Promise<void> {
  if (events.length === 0) return;

  // Use COPY or multi-value INSERT for efficiency
  const values: unknown[] = [];
  const placeholders: string[] = [];

  events.forEach((event, i) => {
    const offset = i * 12;
    placeholders.push(
      `($${offset + 1}, $${offset + 2}, $${offset + 3}, $${offset + 4}, $${offset + 5}, $${offset + 6}, $${offset + 7}, $${offset + 8}, $${offset + 9}, $${offset + 10}, $${offset + 11}, $${offset + 12})`
    );
    values.push(
      event.eventId,
      event.timestamp,
      event.candidateId,
      event.eventType,
      event.searchId,
      event.tenantId,
      event.userIdHash,
      event.dimensions.companyTier,
      event.dimensions.experienceBand,
      event.dimensions.specialty,
      event.rank,
      event.score
    );
  });

  const query = `
    INSERT INTO selection_events (
      event_id, timestamp, candidate_id, event_type,
      search_id, tenant_id, user_id_hash,
      company_tier, experience_band, specialty,
      rank, score
    ) VALUES ${placeholders.join(', ')}
    ON CONFLICT (event_id) DO NOTHING
  `;

  try {
    await pool.query(query, values);
    logger.info({ count: events.length }, 'Batch selection events logged');
  } catch (error) {
    logger.error({ error, count: events.length }, 'Failed to batch log selection events');
  }
}
```

Update `services/hh-search-svc/src/bias/index.ts` to export:
```typescript
export * from './types';
export * from './anonymization';
export * from './selection-events';
```

Create tests `services/hh-search-svc/src/bias/selection-events.test.ts`:

```typescript
import { describe, it, expect } from 'vitest';
import {
  inferCompanyTier,
  inferExperienceBand,
  inferSpecialty,
  createSelectionEvent,
} from './selection-events';

describe('inferCompanyTier', () => {
  it('should identify FAANG companies', () => {
    expect(inferCompanyTier(['Google', 'Startup X'])).toBe('faang');
    expect(inferCompanyTier(['Meta'])).toBe('faang');
    expect(inferCompanyTier(['Amazon Web Services'])).toBe('faang');
  });

  it('should identify enterprise companies', () => {
    expect(inferCompanyTier(['IBM', 'Small Corp'])).toBe('enterprise');
    expect(inferCompanyTier(['Cisco Systems'])).toBe('enterprise');
  });

  it('should identify startups', () => {
    expect(inferCompanyTier(['My Startup', 'Series A Company'])).toBe('startup');
  });

  it('should return other for unknown companies', () => {
    expect(inferCompanyTier(['Unknown Corp', 'Random LLC'])).toBe('other');
    expect(inferCompanyTier([])).toBe('other');
  });
});

describe('inferExperienceBand', () => {
  it('should categorize experience correctly', () => {
    expect(inferExperienceBand(0)).toBe('0-3');
    expect(inferExperienceBand(2)).toBe('0-3');
    expect(inferExperienceBand(3)).toBe('3-7');
    expect(inferExperienceBand(5)).toBe('3-7');
    expect(inferExperienceBand(7)).toBe('7-15');
    expect(inferExperienceBand(10)).toBe('7-15');
    expect(inferExperienceBand(15)).toBe('15+');
    expect(inferExperienceBand(25)).toBe('15+');
  });

  it('should default to 3-7 for undefined', () => {
    expect(inferExperienceBand(undefined)).toBe('3-7');
  });
});

describe('inferSpecialty', () => {
  it('should detect frontend from skills', () => {
    expect(inferSpecialty(['React', 'CSS', 'TypeScript'])).toBe('frontend');
  });

  it('should detect backend from skills', () => {
    expect(inferSpecialty(['Python', 'PostgreSQL', 'Go'])).toBe('backend');
  });

  it('should detect fullstack from mixed skills', () => {
    expect(inferSpecialty(['React', 'TypeScript', 'Python', 'PostgreSQL'])).toBe('fullstack');
  });

  it('should detect from title first', () => {
    expect(inferSpecialty(['Python'], 'Frontend Engineer')).toBe('frontend');
    expect(inferSpecialty(['React'], 'Backend Engineer')).toBe('backend');
  });

  it('should detect devops', () => {
    expect(inferSpecialty(['Kubernetes', 'Docker', 'Terraform'])).toBe('devops');
  });

  it('should detect ML', () => {
    expect(inferSpecialty(['PyTorch', 'TensorFlow', 'scikit-learn'])).toBe('ml');
  });

  it('should return other for unknown skills', () => {
    expect(inferSpecialty(['Management', 'Communication'])).toBe('other');
  });
});

describe('createSelectionEvent', () => {
  it('should create a valid selection event', () => {
    const event = createSelectionEvent(
      'shown',
      'cand-123',
      'search-456',
      'tenant-abc',
      'user-hash-xyz',
      {
        companies: ['Google', 'Startup X'],
        yearsExperience: 8,
        skills: ['Python', 'AWS', 'Docker'],
        title: 'Senior Backend Engineer',
        rank: 5,
        score: 0.85,
      }
    );

    expect(event.eventType).toBe('shown');
    expect(event.candidateId).toBe('cand-123');
    expect(event.searchId).toBe('search-456');
    expect(event.tenantId).toBe('tenant-abc');
    expect(event.dimensions.companyTier).toBe('faang');
    expect(event.dimensions.experienceBand).toBe('7-15');
    expect(event.dimensions.specialty).toBe('backend');
    expect(event.rank).toBe(5);
    expect(event.score).toBe(0.85);
    expect(event.eventId).toBeDefined();
    expect(event.timestamp).toBeInstanceOf(Date);
  });
});
```
  </action>
  <verify>
    npx tsc --noEmit --project services/hh-search-svc/tsconfig.json && cd services/hh-search-svc && npm test -- --grep "selection" --run
  </verify>
  <done>
    Selection event logging module created with dimension inference, all tests pass.
  </done>
</task>

<task type="auto">
  <name>Task 3: Create Fairlearn bias metrics worker</name>
  <files>scripts/bias_metrics_worker.py</files>
  <action>
Create `scripts/bias_metrics_worker.py`:

```python
#!/usr/bin/env python3
"""
Bias Metrics Worker for BIAS-03/BIAS-04

Computes selection rates and impact ratios using Fairlearn.
Designed to run periodically (cron/Cloud Scheduler) to update
bias metrics in the admin dashboard.

Usage:
    python bias_metrics_worker.py --days 30 --dimension company_tier
    python bias_metrics_worker.py --days 7 --all-dimensions
"""

import os
import sys
import json
import logging
import argparse
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple, Any

import psycopg2
import numpy as np
import pandas as pd
from scipy import stats

# Fairlearn imports
from fairlearn.metrics import (
    MetricFrame,
    selection_rate,
    demographic_parity_ratio,
    demographic_parity_difference,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger('bias_metrics_worker')

# Minimum sample size for statistical validity
MIN_SAMPLE_SIZE = 20

# Four-fifths rule threshold
ADVERSE_IMPACT_THRESHOLD = 0.8


def get_db_connection():
    """Get PostgreSQL connection from environment."""
    return psycopg2.connect(
        host=os.environ.get('POSTGRES_HOST', 'localhost'),
        port=os.environ.get('POSTGRES_PORT', '5432'),
        database=os.environ.get('POSTGRES_DB', 'headhunter'),
        user=os.environ.get('POSTGRES_USER', 'headhunter'),
        password=os.environ.get('POSTGRES_PASSWORD', 'headhunter'),
        sslmode=os.environ.get('POSTGRES_SSLMODE', 'prefer'),
    )


def fetch_selection_events(
    conn,
    start_date: datetime,
    end_date: datetime,
    tenant_id: Optional[str] = None
) -> pd.DataFrame:
    """
    Fetch selection events from PostgreSQL for the given date range.

    Returns DataFrame with columns:
    - candidate_id, event_type, company_tier, experience_band, specialty
    """
    query = """
        SELECT
            candidate_id,
            event_type,
            company_tier,
            experience_band,
            specialty,
            search_id,
            timestamp
        FROM selection_events
        WHERE timestamp >= %s AND timestamp < %s
    """
    params = [start_date, end_date]

    if tenant_id:
        query += " AND tenant_id = %s"
        params.append(tenant_id)

    query += " ORDER BY timestamp"

    df = pd.read_sql(query, conn, params=params)
    logger.info(f"Fetched {len(df)} selection events")
    return df


def compute_selection_rates(
    events: pd.DataFrame,
    dimension: str,
    baseline_event: str = 'shown',
    selection_event: str = 'shortlisted'
) -> Dict[str, Any]:
    """
    Compute selection rates for a given dimension using Fairlearn.

    Args:
        events: DataFrame of selection events
        dimension: Column to group by (company_tier, experience_band, specialty)
        baseline_event: Event type for denominator (who was shown)
        selection_event: Event type for numerator (who was selected)

    Returns:
        Dict with selection rates, impact ratios, and warnings
    """
    # Get unique candidates by event type
    shown = events[events['event_type'] == baseline_event]['candidate_id'].unique()
    selected = events[events['event_type'] == selection_event]['candidate_id'].unique()

    if len(shown) == 0:
        logger.warning("No 'shown' events in dataset")
        return {'error': 'No baseline events'}

    # Build dataset for Fairlearn
    # y_true = was the candidate selected?
    # y_pred = was the candidate shown? (always 1 for our analysis)
    # sensitive_features = dimension value

    # Get dimension values for shown candidates
    shown_candidates = events[
        (events['event_type'] == baseline_event)
    ].drop_duplicates(subset=['candidate_id'])

    # Mark which were selected
    shown_candidates = shown_candidates.copy()
    shown_candidates['selected'] = shown_candidates['candidate_id'].isin(selected).astype(int)

    # For Fairlearn: y_true = selected, y_pred = 1 (shown)
    y_true = shown_candidates['selected'].values
    y_pred = np.ones(len(shown_candidates))
    sensitive_features = shown_candidates[dimension].values

    # Compute metrics with MetricFrame
    mf = MetricFrame(
        metrics={'selection_rate': selection_rate},
        y_true=y_true,
        y_pred=y_pred,
        sensitive_features=sensitive_features
    )

    # Get per-group metrics
    group_rates = mf.by_group['selection_rate'].to_dict()
    overall_rate = mf.overall['selection_rate']

    # Compute impact ratios (four-fifths rule)
    max_rate = max(group_rates.values()) if group_rates else 0
    impact_ratios = {}
    adverse_impact_groups = []

    for group, rate in group_rates.items():
        ratio = rate / max_rate if max_rate > 0 else 0
        impact_ratios[group] = ratio

        if ratio < ADVERSE_IMPACT_THRESHOLD:
            adverse_impact_groups.append(group)

    # Get sample sizes per group
    sample_sizes = shown_candidates.groupby(dimension).size().to_dict()

    # Flag groups with insufficient samples
    low_sample_groups = [
        group for group, size in sample_sizes.items()
        if size < MIN_SAMPLE_SIZE
    ]

    # Statistical significance testing for small samples
    significance_tests = {}
    for group in group_rates.keys():
        group_data = shown_candidates[shown_candidates[dimension] == group]
        if len(group_data) >= MIN_SAMPLE_SIZE:
            # Chi-square test vs overall rate
            observed = [group_data['selected'].sum(), len(group_data) - group_data['selected'].sum()]
            expected_rate = overall_rate
            expected = [len(group_data) * expected_rate, len(group_data) * (1 - expected_rate)]

            if min(expected) >= 5:  # Chi-square validity
                chi2, p_value = stats.chisquare(observed, expected)
                significance_tests[group] = {
                    'chi2': float(chi2),
                    'p_value': float(p_value),
                    'significant': p_value < 0.05
                }
            else:
                # Use Fisher's exact test for small expected counts
                # Construct 2x2 table: group vs rest
                rest_selected = y_true.sum() - group_data['selected'].sum()
                rest_not_selected = len(y_true) - y_true.sum() - (len(group_data) - group_data['selected'].sum())
                table = [
                    [int(group_data['selected'].sum()), int(len(group_data) - group_data['selected'].sum())],
                    [int(rest_selected), int(rest_not_selected)]
                ]
                _, p_value = stats.fisher_exact(table)
                significance_tests[group] = {
                    'test': 'fisher_exact',
                    'p_value': float(p_value),
                    'significant': p_value < 0.05
                }

    return {
        'dimension': dimension,
        'period': {
            'start': events['timestamp'].min().isoformat() if len(events) > 0 else None,
            'end': events['timestamp'].max().isoformat() if len(events) > 0 else None,
        },
        'overall_rate': float(overall_rate),
        'selection_rates': {k: float(v) for k, v in group_rates.items()},
        'impact_ratios': {k: float(v) for k, v in impact_ratios.items()},
        'sample_sizes': sample_sizes,
        'demographic_parity_ratio': float(mf.ratio()['selection_rate']) if len(group_rates) > 1 else None,
        'demographic_parity_difference': float(mf.difference()['selection_rate']) if len(group_rates) > 1 else None,
        'adverse_impact_detected': len(adverse_impact_groups) > 0,
        'adverse_impact_groups': adverse_impact_groups,
        'low_sample_groups': low_sample_groups,
        'significance_tests': significance_tests,
        'warnings': _generate_warnings(
            adverse_impact_groups,
            low_sample_groups,
            significance_tests
        )
    }


def _generate_warnings(
    adverse_impact_groups: List[str],
    low_sample_groups: List[str],
    significance_tests: Dict[str, Dict]
) -> List[str]:
    """Generate human-readable warnings for the dashboard."""
    warnings = []

    if adverse_impact_groups:
        groups_str = ', '.join(adverse_impact_groups)
        warnings.append(
            f"Potential adverse impact detected for: {groups_str}. "
            f"These groups have selection rates below 80% of the highest group."
        )

    if low_sample_groups:
        groups_str = ', '.join(low_sample_groups)
        warnings.append(
            f"Low sample size (<{MIN_SAMPLE_SIZE}) for: {groups_str}. "
            f"Impact ratios may not be statistically meaningful."
        )

    # Check for statistically significant differences
    significant_groups = [
        group for group, test in significance_tests.items()
        if test.get('significant', False)
    ]
    if significant_groups:
        groups_str = ', '.join(significant_groups)
        warnings.append(
            f"Statistically significant selection rate differences found for: {groups_str}."
        )

    return warnings


def compute_all_dimensions(
    events: pd.DataFrame,
    baseline_event: str = 'shown',
    selection_event: str = 'shortlisted'
) -> Dict[str, Any]:
    """Compute metrics for all three dimensions."""
    results = {}

    for dimension in ['company_tier', 'experience_band', 'specialty']:
        results[dimension] = compute_selection_rates(
            events, dimension, baseline_event, selection_event
        )

    # Compute aggregate warnings
    all_warnings = []
    for dim_result in results.values():
        if 'warnings' in dim_result:
            all_warnings.extend(dim_result['warnings'])

    return {
        'computed_at': datetime.utcnow().isoformat(),
        'dimensions': results,
        'all_warnings': all_warnings,
        'any_adverse_impact': any(
            r.get('adverse_impact_detected', False)
            for r in results.values()
        )
    }


def save_metrics_to_db(conn, metrics: Dict[str, Any], tenant_id: str) -> None:
    """Save computed metrics to PostgreSQL for dashboard consumption."""
    query = """
        INSERT INTO bias_metrics (
            tenant_id, computed_at, metrics_json
        ) VALUES (%s, %s, %s)
    """

    with conn.cursor() as cur:
        cur.execute(query, [
            tenant_id,
            datetime.utcnow(),
            json.dumps(metrics)
        ])
    conn.commit()
    logger.info(f"Saved metrics for tenant {tenant_id}")


def main():
    parser = argparse.ArgumentParser(description='Compute bias metrics')
    parser.add_argument('--days', type=int, default=30, help='Days to look back')
    parser.add_argument('--dimension', type=str, help='Specific dimension to compute')
    parser.add_argument('--all-dimensions', action='store_true', help='Compute all dimensions')
    parser.add_argument('--tenant-id', type=str, help='Filter by tenant')
    parser.add_argument('--output', type=str, help='Output file (JSON)')
    parser.add_argument('--save-to-db', action='store_true', help='Save results to database')

    args = parser.parse_args()

    end_date = datetime.utcnow()
    start_date = end_date - timedelta(days=args.days)

    logger.info(f"Computing bias metrics for {start_date} to {end_date}")

    conn = get_db_connection()

    try:
        events = fetch_selection_events(conn, start_date, end_date, args.tenant_id)

        if len(events) == 0:
            logger.warning("No selection events found for the period")
            print(json.dumps({'error': 'No data', 'period': {'days': args.days}}))
            return

        if args.all_dimensions:
            results = compute_all_dimensions(events)
        elif args.dimension:
            results = compute_selection_rates(events, args.dimension)
        else:
            results = compute_all_dimensions(events)

        # Output results
        output_json = json.dumps(results, indent=2, default=str)
        print(output_json)

        if args.output:
            with open(args.output, 'w') as f:
                f.write(output_json)
            logger.info(f"Results saved to {args.output}")

        if args.save_to_db:
            tenant_id = args.tenant_id or 'default'
            save_metrics_to_db(conn, results, tenant_id)

    finally:
        conn.close()


if __name__ == '__main__':
    main()
```
  </action>
  <verify>
    python3 -c "import scripts.bias_metrics_worker" 2>/dev/null || python3 -c "
import sys
sys.path.insert(0, 'scripts')
# Just check syntax, don't import (missing deps)
with open('scripts/bias_metrics_worker.py') as f:
    compile(f.read(), 'bias_metrics_worker.py', 'exec')
print('Syntax OK')
"
  </verify>
  <done>
    Fairlearn bias metrics worker created with selection rate computation, impact ratio calculation, and statistical significance testing.
  </done>
</task>

</tasks>

<verification>
- Fairlearn in requirements: `grep "fairlearn" scripts/requirements.txt`
- Selection events module compiles: `npx tsc --noEmit --project services/hh-search-svc/tsconfig.json`
- Selection events tests pass: `cd services/hh-search-svc && npm test -- --grep "selection" --run`
- Python worker syntax valid: `python3 -m py_compile scripts/bias_metrics_worker.py`
</verification>

<success_criteria>
1. Fairlearn 0.13.0+ added to requirements.txt
2. Selection event types and dimension inference implemented
3. logSelectionEvent and logSelectionEventsBatch functions work
4. bias_metrics_worker.py computes selection rates per dimension
5. Impact ratio (four-fifths rule) calculation with 0.8 threshold
6. Statistical significance testing (chi-square, Fisher's exact) for small samples
7. All 10+ tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/14-bias-reduction/14-03-SUMMARY.md`
</output>
