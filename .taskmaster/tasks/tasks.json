{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Set up Ollama with Llama 3.1 8b",
        "description": "Install and configure Ollama locally, pull the llama3.1:8b model for LLM-powered data processing - COMPLETED",
        "status": "done",
        "dependencies": [],
        "priority": "high",
        "details": "Successfully installed Ollama on Mac and pulled llama3.1:8b model (4.9 GB). Verified local LLM functionality with test prompts. System is now ready for processing unstructured candidate data including resume analysis and recruiter comment synthesis.",
        "testStrategy": "Completed: Ran ollama run llama3.1:8b, verified model loads correctly and responds accurately to basic prompts. Model is operational and ready for production use.",
        "subtasks": []
      },
      {
        "id": 2,
        "title": "Create LLM prompts for resume analysis",
        "description": "Design and implement prompts for Llama 3.1 8b to analyze resume content and extract career insights",
        "status": "done",
        "priority": "high",
        "dependencies": [
          1
        ],
        "details": "Create structured prompts that extract: career trajectory, leadership scope, company pedigree, skill assessment, cultural signals from resume text",
        "testStrategy": "Test prompts with sample resume data and validate JSON output structure"
      },
      {
        "id": 3,
        "title": "Create LLM prompts for recruiter comments analysis",
        "description": "Design prompts to synthesize insights from recruiter comments and qualitative feedback",
        "status": "done",
        "priority": "high",
        "dependencies": [
          1
        ],
        "details": "Create prompts that analyze recruiter notes, identify strengths/red flags, extract leadership insights, and generate structured takeaways",
        "testStrategy": "Test with sample recruiter comments and validate insight extraction quality"
      },
      {
        "id": 4,
        "title": "Implement Python LLM processing pipeline",
        "description": "Build Python script that integrates Ollama API to process unstructured data into structured JSON profiles",
        "status": "done",
        "priority": "high",
        "dependencies": [
          2,
          3
        ],
        "details": "Create llm_processor.py with functions to: read CSV data, call Ollama API with prompts, parse LLM responses, generate structured JSON profiles",
        "testStrategy": "Process sample candidate data and validate output JSON structure matches requirements",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Ollama API client integration",
            "description": "Create Python functions to interact with Ollama API for LLM processing",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 2,
            "title": "Implement CSV data loading and preprocessing",
            "description": "Build functions to load candidate and comment data from CSV files with proper data cleaning",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 3,
            "title": "Create LLM prompt orchestration system",
            "description": "Build system to dynamically select and execute appropriate prompts for different data types",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          },
          {
            "id": 4,
            "title": "Implement JSON output parsing and validation",
            "description": "Build robust parsing system for LLM responses with error handling and data validation",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 4
          }
        ]
      },
      {
        "id": 5,
        "title": "Implement resume text extraction",
        "description": "Add functionality to extract text from resume files (PDF, DOCX, images) for LLM analysis",
        "status": "done",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "details": "Integrate PyPDF2, python-docx, and OCR libraries to extract text from resume files before LLM processing",
        "testStrategy": "Test text extraction from various resume formats and verify quality",
        "subtasks": [
          {
            "id": 1,
            "title": "Integrate PDF text extraction with PyPDF2",
            "description": "Add PyPDF2 library and implement PDF text extraction functionality",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 2,
            "title": "Add DOCX text extraction with python-docx",
            "description": "Implement Microsoft Word document text extraction functionality",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          },
          {
            "id": 3,
            "title": "Implement OCR for image-based resumes",
            "description": "Add Tesseract OCR functionality for extracting text from scanned resume images",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 5
          }
        ]
      },
      {
        "id": 6,
        "title": "Set up Google Cloud Platform infrastructure",
        "description": "Enable Vertex AI, Firestore, Cloud Storage, and Vector Search in GCP project",
        "status": "done",
        "priority": "high",
        "dependencies": [],
        "details": "Create GCP project, enable required APIs, set up service accounts, configure Firebase Hosting and Functions",
        "testStrategy": "Verify all required services are enabled and accessible via API",
        "subtasks": [
          {
            "id": 1,
            "title": "Create GCP project and enable core APIs",
            "description": "Set up new GCP project and enable Vertex AI, Firestore, Cloud Storage, and Cloud Functions APIs",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 2,
            "title": "Configure service accounts and IAM permissions",
            "description": "Create service accounts with appropriate permissions for Vertex AI, Firestore, and Cloud Storage access",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          },
          {
            "id": 3,
            "title": "Set up Firebase Hosting and Functions",
            "description": "Initialize Firebase project and configure Hosting and Cloud Functions for the web application",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 6
          }
        ]
      },
      {
        "id": 7,
        "title": "Implement quality validation system",
        "description": "Create evaluation metrics and validation system for LLM analysis accuracy and consistency",
        "status": "done",
        "priority": "medium",
        "dependencies": [
          4
        ],
        "details": "Implement JSON schema validation, output quality scoring, and fallback mechanisms for LLM responses",
        "testStrategy": "Test validation system with known good/bad outputs and verify error handling",
        "subtasks": [
          {
            "id": 1,
            "title": "Create JSON schema validation for LLM outputs",
            "description": "Define and implement JSON schema validation for structured LLM response parsing",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 7
          }
        ]
      },
      {
        "id": 8,
        "title": "Build Cloud Function for data enrichment",
        "description": "Create Cloud Function that processes LLM-generated profiles with Vertex AI Gemini for additional enrichment",
        "status": "done",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "details": "Implement Node.js/TypeScript Cloud Function that triggers on GCS uploads, calls Vertex AI Gemini, and stores results in Firestore",
        "testStrategy": "Test end-to-end flow from file upload to enriched profile storage",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Cloud Function boilerplate and deployment setup",
            "description": "Set up basic Cloud Function structure with TypeScript and deployment configuration",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 8
          }
        ]
      },
      {
        "id": 9,
        "title": "Implement Vector Search integration",
        "description": "Set up Vertex AI Vector Search for semantic similarity matching of candidate profiles",
        "status": "done",
        "priority": "medium",
        "dependencies": [
          6
        ],
        "details": "Create vector embeddings from candidate profiles, configure Vector Search index, implement similarity search functionality",
        "testStrategy": "Test vector search with sample profiles and verify relevant results",
        "subtasks": [
          {
            "id": 1,
            "title": "Set up Vertex AI Vector Search index",
            "description": "Create and configure Vector Search index for candidate profile embeddings",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 9
          }
        ]
      },
      {
        "id": 10,
        "title": "Build search API endpoint",
        "description": "Create Cloud Function API that accepts job descriptions and returns ranked candidate matches",
        "status": "done",
        "priority": "high",
        "dependencies": [
          8,
          9
        ],
        "details": "Implement semantic search logic, ranking algorithm, and JSON response formatting for candidate matches with rationale",
        "testStrategy": "Test API with sample job descriptions and validate match quality and ranking",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement semantic search logic and ranking algorithm",
            "description": "Build the core search algorithm that matches job descriptions to candidate profiles using embeddings",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 10
          }
        ]
      },
      {
        "id": 11,
        "title": "Create React search interface",
        "description": "Build simple web UI for job description input and candidate results display",
        "status": "done",
        "priority": "high",
        "dependencies": [
          10
        ],
        "details": "Create React app with: JD input form, results display with candidate cards, 'Why they're a match' explanations, deployed to Firebase Hosting",
        "testStrategy": "Test complete user flow from JD input to candidate selection",
        "subtasks": [
          {
            "id": 1,
            "title": "Create React app structure and basic components",
            "description": "Set up React application with routing, state management, and core component structure",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 11
          }
        ]
      },
      {
        "id": 12,
        "title": "Implement authentication and security",
        "description": "Add secure access controls and authentication for the search interface",
        "status": "done",
        "priority": "medium",
        "dependencies": [
          11
        ],
        "details": "Implement Firebase Authentication, secure API calls, data access controls, and privacy protections",
        "testStrategy": "Test authentication flows and verify secure data handling",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Firebase Authentication setup",
            "description": "Configure Firebase Auth with secure login/logout flows and user session management",
            "details": "",
            "status": "done",
            "dependencies": [],
            "parentTaskId": 12
          }
        ]
      },
      {
        "id": 13,
        "title": "Configure Together AI API Integration",
        "description": "Set up Together AI API access and create reusable client wrapper for chat completions with Llama 3.1 8B Instruct Turbo model",
        "details": "Configure Together AI integration for production use:\n\n1. **Implementation**:\n   - Create TogetherAIClient class with async methods using aiohttp\n   - Set up environment variable TOGETHER_API_KEY\n   - Configure model endpoint: meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo\n   - Add retry logic with exponential backoff (3 retries, 2-4-8 second delays)\n   - Implement rate limiting (100 requests/minute default)\n   - Add circuit breaker pattern for API failures\n   - Create cost estimation methods based on token usage\n   - Implement request/response logging for debugging\n\n2. **Configuration**:\n   - Store API key securely in environment variables\n   - Configure timeout and retry settings\n   - Set up logging for API interactions\n\n3. **Error Handling**:\n   - Handle API rate limits gracefully\n   - Implement circuit breaker for system stability\n   - Add fallback mechanisms for API failures",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_together_ai_client.py):\n   - Test client initialization with/without API key\n   - Test chat_completion with mocked responses\n   - Test retry logic with simulated failures (verify 3 attempts)\n   - Test rate limiting with 100+ concurrent requests\n   - Test circuit breaker triggers after 5 consecutive failures\n   - Test cost estimation calculations\n\n2. **Integration Tests** (test_together_ai_integration.py):\n   - Test real API connection (use test key)\n   - Test with minimal tokens (5 max) to minimize cost\n   - Test timeout handling\n\n3. **Test Execution**:\n   - Run: `pytest tests/test_together_ai_client.py -v`\n   - Ensure 100% test coverage for critical paths",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Recruiter-Grade Prompt Templates",
        "description": "Create comprehensive prompt engineering system for candidate analysis with structured JSON output schemas",
        "details": "Develop prompt template system:\n\n1. **Implementation**:\n   - Create PromptBuilder class with methods for each analysis type\n   - Define Pydantic/TypedDict models for expected JSON outputs\n   - Implement prompt templates for:\n     * Resume analysis (education, experience, skills)\n     * Recruiter comment analysis\n     * Market insights generation\n     * Cultural assessment\n     * Executive summary creation\n   - Add JSON repair prompts for malformed responses\n   - Include few-shot examples in prompts\n   - Implement prompt versioning system\n\n2. **JSON Schema Design**:\n   - Define strict schemas for each analysis type\n   - Ensure consistent field naming\n   - Add validation rules for data types and ranges\n\n3. **Optimization**:\n   - Keep prompts under token limits\n   - Use clear, concise instructions\n   - Include examples for better output quality",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_prompt_templates.py):\n   - Test each prompt template generates valid output\n   - Test prompt length stays under token limits\n   - Test few-shot examples are properly formatted\n   - Test prompt versioning system\n\n2. **Schema Tests** (test_json_schemas.py):\n   - Test Pydantic model validation\n   - Test required fields are present\n   - Test data type validation\n   - Test nested object validation\n\n3. **Repair Tests** (test_json_repair.py):\n   - Test repair with missing brackets\n   - Test repair with invalid quotes\n   - Test repair with malformed arrays\n\n4. **Test Execution**:\n   - Run: `pytest tests/test_prompt_templates.py -v`\n   - Run: `pytest tests/test_json_schemas.py -v`",
        "priority": "high",
        "dependencies": [
          13
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 15,
        "title": "Build Firestore Streaming Pipeline",
        "description": "Implement direct streaming of processed candidate profiles to Firebase Firestore with batch optimization",
        "details": "Create Firestore streaming pipeline:\n\n1. **Implementation**:\n   - Initialize Firebase Admin SDK with service account\n   - Implement FirestoreStreamer class with batch writes\n   - Create collections: candidates/, enriched_profiles/, embeddings/\n   - Implement upsert logic by candidate_id\n   - Add batch size optimization (500 docs per batch)\n   - Create flattened search-ready fields\n   - Implement transaction support for consistency\n   - Add progress tracking and resumption\n\n2. **Data Structure**:\n   - Design document schema for efficient queries\n   - Create indexes for common search patterns\n   - Implement data flattening for search optimization\n\n3. **Error Handling**:\n   - Handle batch write failures\n   - Implement transaction rollback\n   - Add checkpoint/resume capability",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_firestore_streamer.py):\n   - Test FirestoreStreamer initialization\n   - Test batch accumulation logic\n   - Test flush_batch with mocked Firestore\n   - Test upsert behavior with existing documents\n   - Test flattening logic for search fields\n\n2. **Integration Tests** (test_firestore_integration.py):\n   - Test with Firestore emulator\n   - Test batch write performance (1000 samples)\n   - Test transaction rollback scenarios\n   - Test checkpoint/resume functionality\n\n3. **Security Tests** (test_firestore_security.py):\n   - Test security rules allow writes\n   - Test unauthorized access is blocked\n\n4. **Test Execution**:\n   - Run: `firebase emulators:start --only firestore`\n   - Run: `pytest tests/test_firestore_streamer.py -v`",
        "priority": "high",
        "dependencies": [
          13,
          14
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 16,
        "title": "Create Batch Processing Orchestrator",
        "description": "Build async batch processor to analyze candidates via Together AI with concurrency control and progress tracking",
        "details": "Implement batch processing system:\n\n1. **Implementation**:\n   - Create BatchProcessor class with async/await\n   - Implement CSV/JSON input parsers\n   - Add concurrent processing with semaphore (10 concurrent by default)\n   - Create progress tracking with checkpoint saves\n   - Implement cost estimation and MAX_ESTIMATED_COST safeguards\n   - Add graceful shutdown and resume capability\n   - Create processing statistics and reporting\n\n2. **Concurrency Control**:\n   - Use asyncio semaphore for rate limiting\n   - Implement adaptive concurrency based on API response times\n   - Add memory usage monitoring\n\n3. **Progress Management**:\n   - Save checkpoints after each batch\n   - Enable resume from last checkpoint\n   - Track success/failure statistics",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_batch_processor.py):\n   - Test CSV/JSON parsing\n   - Test semaphore limits concurrency\n   - Test checkpoint saving/loading\n   - Test cost estimation calculations\n   - Test graceful shutdown handling\n\n2. **Async Tests** (test_batch_async.py):\n   - Test async candidate processing\n   - Test concurrent batch operations\n   - Test error handling in async context\n\n3. **Integration Tests** (test_batch_integration.py):\n   - Test with 50-candidate sample\n   - Test resume after interruption\n   - Measure throughput metrics\n\n4. **Test Execution**:\n   - Run: `pytest tests/test_batch_processor.py -v --asyncio-mode=auto`\n   - Verify memory usage stays within limits",
        "priority": "high",
        "dependencies": [
          13,
          14,
          15
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 17,
        "title": "Implement JSON Validation and Repair System",
        "description": "Create robust JSON parsing with schema validation, automatic repair, and quarantine for malformed responses",
        "details": "Build JSON validation system:\n\n1. **Implementation**:\n   - Create JSONValidator class with jsonschema/Pydantic\n   - Implement parse attempts with code fence stripping\n   - Add repair mechanism using Together AI\n   - Create quarantine system for unfixable responses\n   - Implement validation metrics and reporting\n   - Add schema versioning support\n\n2. **Repair Logic**:\n   - Strip markdown code fences\n   - Fix common JSON syntax errors\n   - Use AI to repair complex malformations\n   - Maximum 3 repair attempts before quarantine\n\n3. **Quarantine System**:\n   - Store failed responses for manual review\n   - Track error patterns\n   - Generate repair statistics",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_json_validator.py):\n   - Test valid JSON parsing\n   - Test code fence removal\n   - Test schema validation with Pydantic\n   - Test missing field detection\n\n2. **Repair Tests** (test_json_repair.py):\n   - Test repair with missing brackets\n   - Test repair with invalid quotes\n   - Test repair with truncated JSON\n   - Test max repair attempts\n\n3. **Quarantine Tests** (test_quarantine.py):\n   - Test quarantine after failed repairs\n   - Test quarantine data structure\n   - Test retrieval from quarantine\n\n4. **Test Execution**:\n   - Run: `pytest tests/test_json_validator.py -v`\n   - Ensure >95% parse success rate",
        "priority": "high",
        "dependencies": [
          14
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 18,
        "title": "Set up Cloud Functions for Search and CRUD APIs",
        "description": "Deploy Firebase Cloud Functions for candidate CRUD operations, job management, and semantic search endpoints",
        "details": "Implement Cloud Functions API:\n\n1. **Implementation**:\n   - Set up TypeScript Cloud Functions project structure\n   - Implement CRUD endpoints for candidates and jobs\n   - Create semantic search endpoint\n   - Add authentication middleware\n   - Implement request validation with Zod\n   - Add CORS configuration\n   - Create error handling and logging\n\n2. **API Endpoints**:\n   - GET/POST/PUT/DELETE /candidates\n   - GET/POST /jobs\n   - POST /search/semantic\n   - GET /candidates/:id/similar\n\n3. **Security**:\n   - Implement Firebase Authentication checks\n   - Add rate limiting\n   - Validate all inputs with Zod schemas",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (Jest/TypeScript):\n   - Test CRUD operations with mocked Firestore\n   - Test authentication middleware\n   - Test request validation with Zod\n   - Test error handling\n\n2. **Integration Tests** (with emulator):\n   - Test with Firestore emulator\n   - Test search functionality\n   - Test CORS headers\n\n3. **Load Tests**:\n   - Test with 100 concurrent requests\n   - Measure response times\n   - Test rate limiting\n\n4. **Test Execution**:\n   - Run: `npm test` for unit tests\n   - Run: `npm run test:integration` with emulator",
        "priority": "medium",
        "dependencies": [
          15
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 19,
        "title": "Implement Embedding Generation Service",
        "description": "Create pluggable embedding service supporting Vertex AI initially with migration path to Together AI embeddings",
        "details": "Build embedding service:\n\n1. **Implementation**:\n   - Create EmbeddingService interface\n   - Implement VertexAIEmbeddingProvider (text-embedding-004)\n   - Add TogetherAIEmbeddingProvider stub\n   - Implement deterministic fallback for dev\n   - Create batch embedding generation\n   - Store embeddings in Firestore embeddings/ collection\n   - Add caching layer for repeated texts\n\n2. **Provider Architecture**:\n   - Design pluggable provider interface\n   - Support multiple embedding models\n   - Enable easy provider switching\n\n3. **Optimization**:\n   - Batch requests for efficiency\n   - Cache frequently used embeddings\n   - Implement dimension reduction if needed",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_embedding_service.py):\n   - Test provider initialization\n   - Test single text embedding\n   - Test batch embedding generation\n   - Test cache hit/miss scenarios\n   - Test fallback mechanism\n\n2. **Provider Tests** (test_embedding_providers.py):\n   - Test Vertex AI provider (mocked)\n   - Test Together AI provider stub\n   - Test deterministic fallback\n   - Verify embedding dimensions (768)\n\n3. **Integration Tests** (test_embedding_integration.py):\n   - Test with real API (limited calls)\n   - Test Firestore storage\n   - Test similarity calculations\n\n4. **Test Execution**:\n   - Run: `pytest tests/test_embedding_service.py -v`\n   - Test embedding quality with known pairs",
        "priority": "medium",
        "dependencies": [
          18
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 20,
        "title": "Build React Search Interface",
        "description": "Create secure web application for recruiters to search candidates using job descriptions with ranked results",
        "details": "Develop React frontend:\n\n1. **Implementation**:\n   - Set up React app with TypeScript and Tailwind CSS\n   - Implement Firebase Authentication (Google Sign-In)\n   - Create job description input interface\n   - Build candidate results component with cards\n   - Add 'Why they match' rationale display\n   - Implement filtering and sorting options\n   - Add candidate shortlisting functionality\n   - Deploy to Firebase Hosting\n\n2. **UI Components**:\n   - SearchBar with job description input\n   - CandidateCard with profile summary\n   - MatchRationale component\n   - FilterPanel for refinement\n   - ShortlistManager for saved candidates\n\n3. **State Management**:\n   - Use React Context for auth state\n   - Implement search state management\n   - Add shortlist persistence",
        "testStrategy": "Test Suite:\n1. **Component Tests** (React Testing Library):\n   - Test SearchInterface renders\n   - Test job description input\n   - Test search button triggers API call\n   - Test results display\n   - Test candidate card interactions\n\n2. **Integration Tests** (Cypress):\n   - Test authentication flow\n   - Test search workflow\n   - Test shortlisting\n   - Test responsive design\n\n3. **Accessibility Tests**:\n   - Test WCAG 2.1 AA compliance\n   - Test keyboard navigation\n   - Test screen reader support\n\n4. **Test Execution**:\n   - Run: `npm test` for unit tests\n   - Run: `npm run test:e2e` for Cypress",
        "priority": "medium",
        "dependencies": [
          18,
          19
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 21,
        "title": "Implement Authentication and Access Control",
        "description": "Set up Firebase Authentication with role-based access control and admin management interface",
        "details": "Configure authentication system:\n\n1. **Implementation**:\n   - Configure Firebase Authentication with Google provider\n   - Create Firestore allowed_users collection\n   - Implement role-based middleware (admin, recruiter, viewer)\n   - Build admin UI for user management\n   - Add audit logging for sensitive operations\n   - Implement session management\n   - Create Firestore security rules\n\n2. **Role Management**:\n   - Define roles: admin, recruiter, viewer\n   - Implement role-based UI rendering\n   - Add permission checks on all API endpoints\n\n3. **Audit System**:\n   - Log all data access\n   - Track user actions\n   - Generate audit reports",
        "testStrategy": "Test Suite:\n1. **Backend Tests** (test_auth_middleware.ts):\n   - Test token verification\n   - Test role-based access control\n   - Test audit logging\n   - Test session management\n\n2. **Frontend Tests** (test_auth_components.tsx):\n   - Test Google Sign-In flow\n   - Test protected routes\n   - Test admin UI components\n\n3. **Security Rule Tests** (test_security_rules.ts):\n   - Test unauthorized access is blocked\n   - Test role-based permissions\n   - Test audit log access\n\n4. **Test Execution**:\n   - Run: `npm test` for all tests\n   - Run: `firebase emulators:exec --only firestore 'npm test'`",
        "priority": "high",
        "dependencies": [
          20
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 22,
        "title": "Create Cloud Run Worker for Pub/Sub Processing",
        "description": "Deploy Python Cloud Run service to process candidate enrichment requests via Pub/Sub for scalable async processing",
        "details": "Build Cloud Run worker:\n\n1. **Implementation**:\n   - Create Cloud Run Python service with FastAPI\n   - Set up Pub/Sub topic 'candidate-process-requests'\n   - Implement message handler for candidate processing\n   - Add autoscaling configuration (min 1, max 100)\n   - Implement health checks and metrics\n   - Add dead letter queue for failed messages\n   - Create Cloud Scheduler for batch processing\n\n2. **Message Processing**:\n   - Parse Pub/Sub messages for candidate IDs\n   - Fetch candidate data from GCS/Firestore\n   - Process with Together AI\n   - Store results in Firestore\n\n3. **Reliability**:\n   - Implement retry logic\n   - Add dead letter queue\n   - Monitor processing metrics",
        "testStrategy": "Test Suite:\n1. **Unit Tests** (test_pubsub_worker.py):\n   - Test message parsing\n   - Test candidate processing logic\n   - Test retry mechanism\n   - Test dead letter queue\n\n2. **Integration Tests** (test_worker_integration.py):\n   - Test with Pub/Sub emulator\n   - Test GCS integration\n   - Test Firestore writes\n\n3. **Load Tests** (test_worker_load.py):\n   - Test autoscaling behavior\n   - Test memory usage under load\n   - Test graceful shutdown\n\n4. **Test Execution**:\n   - Run: `pytest tests/test_pubsub_worker.py -v`\n   - Test with `gcloud beta emulators pubsub start`",
        "priority": "low",
        "dependencies": [
          16,
          17
        ],
        "status": "done",
        "subtasks": []
      },
      {
        "id": 23,
        "title": "Cloud SQL pgvector setup for semantic search",
        "description": "Configure Cloud SQL with PostgreSQL and pgvector extension for semantic search capabilities as specified in PRD lines 74, 79, and 143",
        "details": "Set up Cloud SQL with pgvector for vector similarity search:\n\n1. **Cloud SQL Instance Configuration**:\n   - Create Cloud SQL PostgreSQL 15+ instance\n   - Enable pgvector extension: `CREATE EXTENSION vector;`\n   - Configure connection pooling and SSL\n   - Set up private IP for VPC connectivity\n   - Configure automated backups and high availability\n\n2. **Database Schema Design**:\n   ```sql\n   CREATE TABLE candidate_embeddings (\n     id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n     candidate_id VARCHAR(255) NOT NULL,\n     embedding vector(768),  -- Dimension matches embedding model\n     model_version VARCHAR(50),\n     created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n     metadata JSONB,\n     UNIQUE(candidate_id, model_version)\n   );\n   \n   CREATE INDEX ON candidate_embeddings USING ivfflat (embedding vector_cosine_ops)\n   WITH (lists = 100);  -- Tune based on dataset size\n   ```\n\n3. **Python Client Implementation**:\n   ```python\n   from sqlalchemy import create_engine\n   from pgvector.sqlalchemy import Vector\n   import numpy as np\n   \n   class PgVectorStore:\n       def __init__(self, connection_string):\n           self.engine = create_engine(connection_string)\n       \n       def store_embedding(self, candidate_id, embedding):\n           # Store embedding with metadata\n           pass\n       \n       def similarity_search(self, query_embedding, k=10):\n           # Perform cosine similarity search\n           pass\n   ```\n\n4. **Migration from Vertex AI Vector Search**:\n   - Export existing embeddings from Firestore\n   - Batch insert into pgvector tables\n   - Update search API to use Cloud SQL\n   - Implement A/B testing for comparison\n\n5. **Performance Optimization**:\n   - Configure HNSW index for better performance\n   - Implement connection pooling\n   - Add read replicas for scaling\n   - Monitor query performance metrics",
        "testStrategy": "Comprehensive testing approach:\n\n1. **Unit Tests** (test_pgvector_store.py):\n   - Test connection establishment\n   - Test embedding insertion and retrieval\n   - Test similarity search with various thresholds\n   - Test batch operations performance\n   - Test error handling for connection failures\n\n2. **Integration Tests**:\n   - Test with actual candidate embeddings\n   - Verify search results match expected candidates\n   - Test concurrent read/write operations\n   - Validate index performance with 10k+ embeddings\n\n3. **Performance Benchmarks**:\n   - Measure query latency (target <100ms for 10k vectors)\n   - Test throughput for batch insertions\n   - Compare search quality with Vertex AI baseline\n   - Load test with concurrent connections\n\n4. **Migration Validation**:\n   - Compare search results between old and new systems\n   - Verify data integrity after migration\n   - Test rollback procedures",
        "status": "done",
        "dependencies": [
          19
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 24,
        "title": "Integrate pgvector with Vector Search API",
        "description": "Replace the Firestore-based vector search implementation in functions/src/vector-search.ts with Cloud SQL pgvector backend, creating a Node.js client bridge while maintaining existing API contracts for frontend compatibility.",
        "details": "Implement pgvector integration for the Vector Search API:\n\n1. **Create Node.js pgvector client package**:\n   - Install required dependencies: `npm install pg @pgvector/pgvector dotenv`\n   - Create `functions/src/pgvector-client.ts` as a TypeScript wrapper around the Python pgvector_store.py functionality\n   - Implement connection pooling using node-postgres (pg) with SSL and Cloud SQL socket support\n   - Add retry logic and circuit breaker patterns for resilience\n\n2. **Implement PgVectorClient class**:\n   ```typescript\n   class PgVectorClient {\n     private pool: Pool;\n     \n     constructor(config: PgVectorConfig) {\n       // Initialize connection pool with Cloud SQL socket factory\n       // Support both TCP and Unix socket connections\n     }\n     \n     async storeEmbedding(candidateId: string, embedding: number[], metadata: any): Promise<string>\n     async searchSimilar(queryEmbedding: number[], threshold: number, limit: number): Promise<SearchResult[]>\n     async getEmbeddingStats(): Promise<EmbeddingStats>\n     async healthCheck(): Promise<HealthStatus>\n   }\n   ```\n\n3. **Modify VectorSearchService (vector-search.ts)**:\n   - Replace Firestore embedding storage with PgVectorClient calls\n   - Update `storeEmbedding()` to use pgvector's upsert_candidate_embedding function\n   - Replace `searchCandidates()` implementation:\n     - Remove Firestore collection queries\n     - Use pgvector's similarity_search function with cosine distance\n     - Maintain the same response format for API compatibility\n   - Update `findSimilarCandidates()` to use pgvector backend\n   - Keep existing API interfaces unchanged (SearchQuery, VectorSearchResult)\n\n4. **Connection configuration**:\n   - Add environment variables for Cloud SQL connection:\n     ```\n     PGVECTOR_HOST (Cloud SQL instance IP or socket path)\n     PGVECTOR_PORT (5432)\n     PGVECTOR_DATABASE (headhunter)\n     PGVECTOR_USER\n     PGVECTOR_PASSWORD\n     PGVECTOR_SSL_MODE (require for production)\n     PGVECTOR_MAX_CONNECTIONS (20)\n     PGVECTOR_IDLE_TIMEOUT_MILLIS (30000)\n     ```\n   - Support Cloud SQL proxy for local development\n   - Implement IAM authentication for production\n\n5. **Performance optimizations**:\n   - Connection pooling with min/max connections\n   - Prepared statements for frequently used queries\n   - Batch embedding operations for bulk inserts\n   - Implement query result caching with TTL\n   - Add request coalescing for duplicate searches\n\n6. **Error handling and monitoring**:\n   - Implement comprehensive error types (ConnectionError, QueryError, TimeoutError)\n   - Add structured logging with correlation IDs\n   - Export metrics for connection pool usage, query latency, and error rates\n   - Implement health check endpoint that verifies pgvector connectivity\n\n7. **Migration path**:\n   - Create feature flag ENABLE_PGVECTOR_SEARCH (default false)\n   - Implement dual-write pattern during transition:\n     - Write to both Firestore and pgvector\n     - Read from pgvector if flag enabled, fallback to Firestore\n   - Add migration script to backfill existing Firestore embeddings to pgvector\n\n8. **REST API bridge** (optional if direct DB connection not preferred):\n   - Create lightweight Express API wrapper around pgvector_store.py\n   - Deploy as Cloud Run service for better scaling\n   - Implement authentication and rate limiting\n   - Use this if TypeScript pgvector libraries prove unstable",
        "testStrategy": "Comprehensive testing strategy:\n\n1. **Unit Tests** (functions/src/__tests__/pgvector-client.test.ts):\n   - Test connection pool initialization and configuration\n   - Mock pg client for testing query construction\n   - Test error handling for connection failures\n   - Verify retry logic and circuit breaker behavior\n   - Test query parameter validation and sanitization\n\n2. **Integration Tests** (functions/src/__tests__/vector-search-pgvector.test.ts):\n   - Set up test database with pgvector extension\n   - Test end-to-end embedding storage and retrieval\n   - Verify similarity search with various thresholds\n   - Test concurrent operations and connection pool limits\n   - Validate performance under load (response time < 100ms for searches)\n   - Test failover behavior when database is unavailable\n\n3. **API Contract Tests**:\n   - Ensure searchCandidates response format unchanged\n   - Verify all existing API endpoints work with pgvector backend\n   - Test backward compatibility with frontend\n   - Validate that existing tests in api-endpoints.test.ts still pass\n\n4. **Performance Tests**:\n   - Benchmark search performance vs Firestore implementation\n   - Load test with 1000+ concurrent searches\n   - Measure connection pool efficiency\n   - Profile memory usage under sustained load\n\n5. **Migration Tests**:\n   - Test dual-write pattern correctness\n   - Verify data consistency between Firestore and pgvector\n   - Test feature flag toggle behavior\n   - Validate rollback procedures",
        "status": "in-progress",
        "dependencies": [
          23,
          10,
          11
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 25,
        "title": "Implement Skill Probability Assessment",
        "description": "Enhance Together AI prompts to include skill confidence scoring and explicit vs inferred skill classification, update candidate profile schema with skill probabilities, and implement skill-aware search ranking algorithm with frontend components to display confidence levels",
        "details": "Implement comprehensive skill probability assessment system:\n\n1. **Prompt Enhancement**:\n   - Extend PromptBuilder class to include skill confidence scoring prompts\n   - Add explicit vs inferred skill classification logic\n   - Create prompts that analyze:\n     * Explicitly stated skills (from resume text)\n     * Inferred skills (from job descriptions and responsibilities)\n     * Contextual skill indicators (projects, achievements)\n     * Skill recency and relevance scoring\n   - Include confidence levels: high (90-100%), medium (70-89%), low (50-69%)\n   - Add industry-specific skill taxonomies\n\n2. **Schema Updates**:\n   - Update Pydantic/TypedDict models for skill probabilities\n   - Add new fields to candidate profile:\n     * skills_explicit: Array of {skill: string, confidence: number, evidence: string[]}\n     * skills_inferred: Array of {skill: string, confidence: number, reasoning: string}\n     * skill_categories: {technical: [], soft: [], domain: []}\n     * skill_timeline: Track skill usage over career\n   - Version schema for backward compatibility\n\n3. **Search Ranking Algorithm**:\n   - Implement weighted skill matching algorithm\n   - Factor in confidence scores for ranking:\n     * Explicit skills weight: 1.0\n     * High confidence inferred: 0.8\n     * Medium confidence inferred: 0.5\n     * Low confidence inferred: 0.3\n   - Create skill similarity matrix using embeddings\n   - Implement fuzzy matching for skill variations\n   - Add skill gap analysis for job requirements\n\n4. **Backend Implementation**:\n   - Create SkillAssessmentService class\n   - Integrate with existing Together AI processor\n   - Update embedding generation to include skill vectors\n   - Modify search API to use skill-aware ranking\n   - Add caching for skill taxonomy lookups\n\n5. **Frontend Components**:\n   - Create SkillConfidenceDisplay component\n   - Implement visual indicators (progress bars, badges)\n   - Add tooltips showing evidence/reasoning\n   - Create skill comparison view for job matches\n   - Implement filter by skill confidence level\n   - Add skill timeline visualization",
        "testStrategy": "Comprehensive testing strategy:\n\n1. **Prompt Testing** (test_skill_prompts.py):\n   - Test skill extraction from various resume formats\n   - Verify confidence scoring accuracy\n   - Test explicit vs inferred classification\n   - Validate skill taxonomy mapping\n   - Test edge cases (ambiguous skills, abbreviations)\n\n2. **Schema Validation** (test_skill_schema.py):\n   - Test Pydantic models with skill probability data\n   - Verify backward compatibility with existing profiles\n   - Test schema migration for existing candidates\n   - Validate confidence score ranges\n\n3. **Ranking Algorithm Tests** (test_skill_ranking.py):\n   - Test weighted matching with known skill sets\n   - Verify confidence-based ranking order\n   - Test fuzzy matching accuracy\n   - Validate skill gap analysis\n   - Performance test with 1000+ candidates\n\n4. **Integration Tests** (test_skill_integration.py):\n   - End-to-end test from resume to skill assessment\n   - Test Together AI prompt responses\n   - Verify Firestore storage of skill data\n   - Test search API with skill-based queries\n\n5. **Frontend Tests** (test_skill_components.tsx):\n   - Test SkillConfidenceDisplay rendering\n   - Verify tooltip content accuracy\n   - Test filter functionality\n   - Validate skill comparison views\n   - Test responsive design on mobile",
        "status": "pending",
        "dependencies": [
          14,
          17,
          19
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 26,
        "title": "End-to-End Integration Testing",
        "description": "Validate complete recruiter workflows including Job Description to Candidate Recommendations, Resume Upload to Similar Candidate Search, authentication and CRUD operations. Test embedding generation and search integration.",
        "details": "Create comprehensive integration testing framework for production readiness:\n\n1. **Test Framework Setup**:\n   - Create `tests/integration/` directory structure\n   - Set up pytest-asyncio for async test support\n   - Configure test fixtures for database seeding and cleanup\n   - Implement test data factory for realistic candidate profiles\n   - Create mock services for external API dependencies\n   - Set up test coverage reporting with pytest-cov\n   - Configure parallel test execution for speed\n\n2. **Workflow Test Suites**:\n   - **Job Description to Recommendations** (test_job_to_candidates.py):\n     * Test job description parsing and skill extraction\n     * Verify embedding generation for job requirements\n     * Test pgvector similarity search for candidate matching\n     * Validate ranking algorithm with skill probabilities\n     * Assert recommendation quality metrics\n   - **Resume Upload to Similar Search** (test_resume_similarity.py):\n     * Test multi-format resume upload (PDF, DOCX, TXT)\n     * Verify Together AI processing pipeline\n     * Test embedding generation and storage\n     * Validate similar candidate retrieval\n     * Assert search result relevance\n\n3. **Authentication & Security Tests** (test_auth_integration.py):\n   - Test Firebase Authentication flow end-to-end\n   - Verify JWT token validation across services\n   - Test role-based access control (RBAC)\n   - Validate API rate limiting\n   - Test CORS configuration\n   - Verify data isolation between tenants\n\n4. **CRUD Operations Testing** (test_crud_operations.py):\n   - Test candidate profile creation with Together AI enrichment\n   - Verify profile update operations and version control\n   - Test bulk operations performance\n   - Validate cascade delete operations\n   - Test transaction rollback scenarios\n   - Verify Firestore and pgvector synchronization\n\n5. **Embedding & Search Integration** (test_vector_search.py):\n   - Test VertexAI embedding generation pipeline\n   - Verify pgvector storage and indexing\n   - Test semantic search accuracy\n   - Validate hybrid search (keyword + vector)\n   - Test search result pagination\n   - Verify search performance under load\n\n6. **Production Readiness Validation**:\n   - **Performance Testing** (test_performance.py):\n     * Load test with 1000+ concurrent users\n     * Stress test API endpoints\n     * Measure response time percentiles (p50, p95, p99)\n     * Test database connection pooling\n   - **Reliability Testing** (test_reliability.py):\n     * Test circuit breaker activation\n     * Verify retry logic with transient failures\n     * Test graceful degradation\n     * Validate error recovery mechanisms\n   - **Data Consistency** (test_consistency.py):\n     * Test eventual consistency between stores\n     * Verify transaction atomicity\n     * Test concurrent update handling\n     * Validate data integrity constraints\n\n7. **Test Automation & CI/CD**:\n   - Create GitHub Actions workflow for integration tests\n   - Set up test environment provisioning scripts\n   - Implement test result reporting to dashboard\n   - Configure automatic rollback on test failure\n   - Create smoke test suite for production deployments",
        "testStrategy": "Comprehensive validation approach:\n\n1. **Environment Setup**:\n   - Provision isolated test GCP project\n   - Deploy test instances of all services\n   - Seed test database with 500+ candidate profiles\n   - Configure test API keys and credentials\n\n2. **Test Execution**:\n   - Run tests in parallel with pytest-xdist\n   - Execute smoke tests first (5 min)\n   - Run full integration suite (30 min)\n   - Perform load testing (15 min)\n   - Generate coverage report (target: >80%)\n\n3. **Validation Criteria**:\n   - All critical paths must pass 100%\n   - API response times < 500ms p95\n   - Search relevance score > 0.85\n   - Zero data consistency errors\n   - No memory leaks over 1-hour run\n   - Error rate < 0.1%\n\n4. **Monitoring & Reporting**:\n   - Real-time test execution dashboard\n   - Automated failure notifications\n   - Performance regression detection\n   - Test flakiness tracking\n   - Historical trend analysis",
        "status": "pending",
        "dependencies": [
          22,
          24,
          25,
          12,
          17
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "created": "2025-09-05T18:18:48.859Z",
      "updated": "2025-09-11T18:00:26.210Z",
      "description": "Tasks for master context"
    }
  }
}