{
  "master": {
    "tasks": [
      {
        "id": 66,
        "title": "Setup Together AI Integration and Environment Configuration",
        "description": "Configure Together AI API access with Qwen 2.5 32B model and set up required environment variables for cloud processing pipeline",
        "details": "Set up TOGETHER_API_KEY environment variable and configure TOGETHER_MODEL_STAGE1 to use Qwen 2.5 32B Instruct. Implement async HTTP client using aiohttp for Together AI chat completions API (https://api.together.xyz/v1/chat/completions). Create configuration validation to ensure API connectivity and model availability. Set up rate limiting and circuit breaker patterns for API reliability. Configure us-central1 region for all GCP resources.",
        "testStrategy": "Unit tests for API configuration validation, integration tests for Together AI connectivity, mock API responses for development testing, verify rate limiting and error handling with actual API calls",
        "priority": "high",
        "dependencies": [],
        "status": "in-progress",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Environment Variables and API Key Validation",
            "description": "Set up TOGETHER_API_KEY environment variable and TOGETHER_MODEL_STAGE1 configuration with validation logic to ensure proper API access",
            "dependencies": [],
            "details": "Create environment configuration module to handle TOGETHER_API_KEY setup and TOGETHER_MODEL_STAGE1 configuration for Qwen 2.5 32B Instruct model. Implement validation functions to verify API key format and accessibility. Add configuration loading with proper error handling for missing or invalid environment variables. Include secure key management practices and validation logging.\n<info added on 2025-09-13T20:19:23.512Z>\nTDD Progress Update: Successfully implemented comprehensive environment configuration validation with focused test coverage (9/9 tests passing in tests/test_env_config.py). Key validations now enforce REGION restriction to us-central1, EMBEDDING_PROVIDER allowlist (gemini default, vertex/local options), RERANK_PROVIDER policy requiring together, and strict TOGETHER_API_KEY requirement in non-test environments. Configuration module (cloud_run_worker/config.py) enhanced with region/provider fields, validate() method enforcement, and to_dict() exposure. Simplified API key retrieval to directly require TOGETHER_API_KEY without Secret Manager fallback per PRD specifications. Next development phase: implement Together client connectivity health-check unit tests and rate limiter/circuit breaker behavior validation using aiohttp mocks.\n</info added on 2025-09-13T20:19:23.512Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Async HTTP Client with Rate Limiting and Circuit Breaker",
            "description": "Create async HTTP client using aiohttp for Together AI API with rate limiting and circuit breaker patterns for reliability",
            "dependencies": [
              "66.1"
            ],
            "details": "Implement async HTTP client using aiohttp for Together AI chat completions API (https://api.together.xyz/v1/chat/completions). Add rate limiting mechanism to respect API limits and prevent throttling. Implement circuit breaker pattern for handling API failures gracefully. Include retry logic with exponential backoff, timeout handling, and proper error classification for transient vs permanent failures.\n<info added on 2025-09-14T14:40:35.920Z>\nCompleted TDD implementation for Together client resiliency with comprehensive test coverage in tests/test_together_client_resilience.py. Tests validate sliding window rate limiting behavior, retry logic with exponential backoff, circuit breaker threshold activation, and basic success path functionality. Fixed critical issues in scripts/together_client.py including proper metrics/logging initialization in __init__, corrected exponential backoff placement within retry loop, and removed misaligned code block. All 3 new resilience tests pass successfully, confirming robust error handling and API reliability patterns are working as designed.\n</info added on 2025-09-14T14:40:35.920Z>",
            "status": "done",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create Configuration Validation Module with Connectivity Tests",
            "description": "Build validation module to test API connectivity, model availability, and overall system configuration health",
            "dependencies": [
              "66.2"
            ],
            "details": "Create comprehensive configuration validation module that tests Together AI API connectivity and Qwen 2.5 32B model availability. Implement health check functions that verify API endpoints, authentication, and model access. Add validation tests for configuration completeness and correctness. Include diagnostic logging and error reporting for configuration issues.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Configure GCP Region Settings for us-central1",
            "description": "Set up GCP region configuration to ensure all resources are deployed in us-central1 region consistently",
            "dependencies": [
              "66.1"
            ],
            "details": "Configure GCP region settings to use us-central1 across all services and resources. Update configuration files and environment variables to enforce regional consistency. Implement region validation checks to ensure all GCP resources are created in the correct region. Add configuration management for regional settings that can be easily maintained and updated.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 67,
        "title": "Implement PostgreSQL with pgvector Setup and Schema",
        "description": "Set up PostgreSQL database with pgvector extension for hybrid search combining vector similarity and full-text search capabilities",
        "details": "Provision Cloud SQL PostgreSQL instance in us-central1 with pgvector extension enabled. Create schema with candidate_embeddings table supporting HNSW indexing for cosine similarity search. Implement Portuguese FTS configuration for text search. Set up hybrid search infrastructure combining pgvector (HNSW/cosine) with PostgreSQL FTS. Create connection pooling and query optimization for p95 â‰¤ 1.2s performance requirement. Include compliance fields (legal_basis, consent_record, transfer_mechanism).",
        "testStrategy": "Database connectivity tests, pgvector extension verification, HNSW index performance tests, FTS configuration validation for Portuguese text, hybrid search query performance benchmarks",
        "priority": "high",
        "dependencies": [
          66
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Provision Cloud SQL PostgreSQL Instance with pgvector Extension",
            "description": "Set up Cloud SQL PostgreSQL instance in us-central1 region with pgvector extension enabled for vector similarity search capabilities",
            "dependencies": [],
            "details": "Create Cloud SQL PostgreSQL instance with appropriate machine type and storage configuration. Enable pgvector extension through database flags or manual installation. Configure network access, SSL certificates, and backup policies. Set up database users and permissions for application access.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create Database Schema with candidate_embeddings Table and HNSW Indexing",
            "description": "Design and implement database schema with candidate_embeddings table optimized for vector similarity search using HNSW indexing",
            "dependencies": [
              "67.1"
            ],
            "details": "Create candidate_embeddings table with vector column for embeddings storage. Implement HNSW index configuration for cosine similarity search optimization. Define primary keys, foreign key relationships, and appropriate data types. Create migration scripts for schema deployment and version control.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Portuguese Full-Text Search (FTS) Configuration",
            "description": "Set up PostgreSQL full-text search configuration optimized for Portuguese language text processing and search capabilities",
            "dependencies": [
              "67.2"
            ],
            "details": "Configure Portuguese text search configuration with appropriate dictionaries and stemming rules. Create GIN indexes for full-text search columns. Set up text search vectors and ranking functions. Test search accuracy with Portuguese text samples and optimize search performance.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Implement Connection Pooling with asyncpg",
            "description": "Set up database connection pooling using asyncpg for optimal performance and resource management",
            "dependencies": [
              "67.1"
            ],
            "details": "Configure asyncpg connection pool with appropriate pool size, timeout settings, and connection lifecycle management. Implement connection health checks and automatic reconnection logic. Set up connection monitoring and metrics collection. Create database service layer with proper error handling and retry mechanisms.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Add Compliance Fields to Database Schema",
            "description": "Extend database schema with GDPR compliance fields including legal_basis, consent_record, and transfer_mechanism",
            "dependencies": [
              "67.2"
            ],
            "details": "Add compliance fields (legal_basis, consent_record, transfer_mechanism) to candidate_embeddings and related tables. Create appropriate constraints, default values, and validation rules. Implement audit trail columns for tracking data processing activities. Update migration scripts to include compliance field additions.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Performance Testing and Optimization for p95 â‰¤ 1.2s Requirement",
            "description": "Conduct comprehensive performance testing and optimization to meet p95 â‰¤ 1.2s query response time requirement",
            "dependencies": [
              "67.2",
              "67.3",
              "67.4"
            ],
            "details": "Create performance test suite for hybrid search queries combining vector similarity and full-text search. Benchmark HNSW index performance with various vector dimensions and dataset sizes. Optimize query execution plans and index configurations. Monitor and tune connection pool settings, query caching, and database parameters to achieve p95 â‰¤ 1.2s performance target.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 68,
        "title": "Develop Single-Pass Candidate Enrichment Processor",
        "description": "Create Python processor using Together AI Qwen 2.5 32B for single-pass candidate profile enrichment with PII-minimizing prompts",
        "details": "Implement intelligent_skill_processor.py with async processing using aiohttp. Create PII-minimizing prompts for candidate analysis returning structured JSON with personal_details, education_analysis, experience_analysis, technical_assessment, market_insights, cultural_assessment, executive_summary, and skill_inference with confidence scores and evidence. Include analysis_confidence scoring and quality_flags. Implement robust JSON parsing with schema validation, retry logic, and auto-repair capabilities targeting <1% quarantine rate. Add processing_metadata tracking.",
        "testStrategy": "Unit tests for prompt generation, JSON schema validation tests, integration tests with Together AI API, performance tests for single-pass processing, validation of PII minimization in prompts",
        "priority": "high",
        "dependencies": [
          66
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create PII-Minimizing Prompt Templates for Together AI",
            "description": "Design and implement prompt templates that minimize personally identifiable information while maintaining analysis quality for candidate enrichment",
            "dependencies": [],
            "details": "Create prompt templates for Together AI Qwen 2.5 32B that focus on skills, experience patterns, and qualifications while avoiding direct PII references. Implement templates for personal_details, education_analysis, experience_analysis, technical_assessment, market_insights, cultural_assessment, and executive_summary sections. Ensure prompts guide the model to return structured JSON responses with confidence scores and evidence extraction.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Async Processing Pipeline with aiohttp",
            "description": "Build asynchronous processing pipeline using aiohttp for efficient API calls to Together AI",
            "dependencies": [
              "68.1"
            ],
            "details": "Implement async HTTP client using aiohttp for Together AI chat completions API calls. Create connection pooling, timeout handling, and concurrent request management. Integrate with existing intelligent_skill_processor.py structure and ensure proper async/await patterns throughout the processing pipeline. Include rate limiting and circuit breaker patterns for API reliability.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build JSON Schema Validation and Auto-Repair System",
            "description": "Implement robust JSON parsing with schema validation, retry logic, and auto-repair capabilities to achieve <1% quarantine rate",
            "dependencies": [
              "68.2"
            ],
            "details": "Create comprehensive JSON schema validation for all response sections (personal_details, education_analysis, experience_analysis, technical_assessment, market_insights, cultural_assessment, executive_summary, skill_inference). Implement auto-repair logic for common JSON formatting issues, retry mechanisms for failed parsing, and fallback strategies. Target <1% quarantine rate through intelligent error recovery and response cleaning.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add Skill Inference Engine with Confidence Scoring",
            "description": "Implement skill inference capabilities with confidence scoring and evidence extraction for comprehensive candidate assessment",
            "dependencies": [
              "68.3"
            ],
            "details": "Build skill inference engine that analyzes candidate profiles and extracts technical and soft skills with confidence scores. Implement evidence extraction to support skill assessments with specific examples from candidate data. Include analysis_confidence scoring system and quality_flags for assessment reliability. Integrate with existing skill taxonomy and ensure structured output format.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Processing Metadata Tracking and Quality Flags",
            "description": "Add comprehensive processing metadata tracking and quality assessment flags for monitoring and debugging",
            "dependencies": [
              "68.4"
            ],
            "details": "Implement processing_metadata tracking system that captures processing timestamps, API response times, retry attempts, confidence scores, and quality metrics. Add quality_flags system for identifying potential issues in processed data. Create logging and monitoring capabilities for tracking processing success rates, performance metrics, and error patterns. Ensure metadata supports debugging and quality assurance processes.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 69,
        "title": "Implement Gemini Embeddings with Pluggable Provider Architecture",
        "description": "Set up embedding generation using Gemini Embeddings as default with Vertex AI as fallback, maintaining provider pluggability",
        "details": "Create VectorSearchService with pluggable embedding providers (gemini|vertex|local via EMBEDDING_PROVIDER env). Implement Gemini Embeddings as primary provider with automatic fallback to Vertex AI text-embedding-004. Generate embeddings from enriched candidate profiles and store in candidate_embeddings collection in PostgreSQL. Ensure deterministic fallback for development environments. Create embedding generation pipeline that processes enriched profiles and updates pgvector indexes.",
        "testStrategy": "Provider switching tests, embedding generation validation, fallback mechanism testing, performance benchmarks for different providers, vector similarity accuracy tests",
        "priority": "medium",
        "dependencies": [
          67,
          68
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Extend Pluggable Provider Interface for Gemini Support",
            "description": "Enhance the existing embedding provider interface to support Gemini Embeddings alongside current vertex and local providers",
            "dependencies": [],
            "details": "Modify embedding-provider.ts interface to include Gemini provider type. Update EMBEDDING_PROVIDER environment variable validation to accept 'gemini' option. Ensure provider interface supports Gemini-specific configuration parameters and authentication methods. Update provider factory pattern to instantiate Gemini provider when selected.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Gemini Embeddings Provider",
            "description": "Create Gemini Embeddings provider implementation as the primary embedding generation option",
            "dependencies": [
              "69.1"
            ],
            "details": "Implement GeminiEmbeddingProvider class following the established provider interface. Configure Gemini API authentication and endpoint connections. Implement text embedding generation using Gemini's embedding models. Add proper error handling and response validation. Ensure compatibility with existing embedding pipeline and vector dimensions expected by pgvector storage.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Configure Automatic Fallback to Vertex AI",
            "description": "Implement deterministic fallback mechanism from Gemini to Vertex AI text-embedding-004 when primary provider fails",
            "dependencies": [
              "69.2"
            ],
            "details": "Create fallback logic that automatically switches to Vertex AI text-embedding-004 when Gemini provider encounters errors or timeouts. Implement deterministic fallback behavior for development environments to ensure consistent testing. Add logging and monitoring for fallback events. Ensure seamless transition between providers without data loss or processing interruption. Configure retry policies and circuit breaker patterns.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Create Embedding Generation Pipeline with pgvector Integration",
            "description": "Build complete embedding generation pipeline that processes enriched candidate profiles and updates pgvector indexes",
            "dependencies": [
              "69.3"
            ],
            "details": "Create embedding generation pipeline that reads enriched candidate profiles and generates embeddings using the configured provider (Gemini with Vertex fallback). Implement batch processing for efficient embedding generation. Store generated embeddings in candidate_embeddings PostgreSQL collection with proper pgvector formatting. Update HNSW indexes for optimal vector similarity search performance. Add pipeline monitoring and error handling for production reliability.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 70,
        "title": "Build Hybrid Search with Together Rerank Integration",
        "description": "Implement hybrid search combining pgvector similarity search with PostgreSQL FTS and Together AI reranking for top-K results",
        "details": "Create search API that performs pgvector HNSW cosine similarity search combined with PostgreSQL FTS for Portuguese text. Implement score fusion algorithm for hybrid results. Integrate Together AI reranking service to rerank top-Kâ‰ˆ200 results down to top-20. Ensure rerank latency â‰¤350ms @Kâ‰¤200. Include explainability evidence per skill match for 'Why match' bullets. Support both English and Portuguese job descriptions. Implement composite scoring with skill_match, confidence, vector_similarity, and experience factors.",
        "testStrategy": "Hybrid search accuracy tests, reranking performance benchmarks, latency validation for p95 â‰¤ 1.2s requirement, Portuguese FTS functionality tests, explainability evidence validation",
        "priority": "high",
        "dependencies": [
          67,
          69
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement pgvector HNSW cosine similarity search",
            "description": "Set up pgvector extension with HNSW indexing for efficient cosine similarity search on candidate embeddings",
            "dependencies": [],
            "details": "Configure pgvector extension in PostgreSQL database. Create HNSW index on candidate embedding vectors for cosine similarity search. Implement search query functions with configurable similarity thresholds and result limits. Optimize index parameters (m, ef_construction) for performance. Add database migration scripts for pgvector setup and index creation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Configure PostgreSQL FTS for Portuguese text search",
            "description": "Set up PostgreSQL Full Text Search with Portuguese language configuration for job description and candidate profile text search",
            "dependencies": [],
            "details": "Configure PostgreSQL FTS with Portuguese text search configuration (portuguese). Create GIN indexes on tsvector columns for job descriptions and candidate profiles. Implement text preprocessing for Portuguese stemming and stop words. Add search ranking functions using ts_rank_cd. Create database schema updates for FTS columns and indexes.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create score fusion algorithm for hybrid results",
            "description": "Implement algorithm to combine and normalize scores from pgvector similarity search and PostgreSQL FTS results",
            "dependencies": [
              "70.1",
              "70.2"
            ],
            "details": "Develop score fusion algorithm using weighted combination of vector similarity scores and FTS ranking scores. Implement score normalization to ensure comparable ranges between different search methods. Create configurable weight parameters for vector vs text search importance. Add result deduplication logic for candidates appearing in both search results. Implement top-K candidate selection from fused results.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Integrate Together AI reranking API for top-K results",
            "description": "Implement Together AI API integration to rerank top-Kâ‰ˆ200 hybrid search results down to top-20 with â‰¤350ms latency",
            "dependencies": [
              "70.3"
            ],
            "details": "Integrate Together AI reranking service using Qwen 2.5 32B model. Implement async API calls with proper error handling and timeout controls. Create reranking prompt templates for job-candidate matching context. Add latency monitoring to ensure â‰¤350ms rerank performance. Implement fallback mechanism when reranking service is unavailable. Add rate limiting and circuit breaker patterns for API reliability.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Build explainability system for skill match evidence",
            "description": "Create system to generate 'Why match' evidence bullets explaining skill matches between candidates and job requirements",
            "dependencies": [
              "70.4"
            ],
            "details": "Implement skill extraction and matching logic for both candidates and job descriptions. Create evidence generation system that identifies specific skill overlaps and experience relevance. Build explanation templates for different match types (exact skill match, related experience, domain expertise). Add confidence scoring for each explanation. Support both English and Portuguese explanation generation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement composite scoring with multiple factors",
            "description": "Create comprehensive scoring system combining skill_match, confidence, vector_similarity, and experience factors",
            "dependencies": [
              "70.5"
            ],
            "details": "Implement composite scoring algorithm incorporating skill_match score, confidence level, vector_similarity score, and experience relevance factors. Create configurable weight parameters for each scoring component. Add normalization logic to ensure consistent score ranges. Implement score explanation breakdown for transparency. Add A/B testing framework for score weight optimization.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Performance optimization for p95 â‰¤ 1.2s with rerank â‰¤350ms",
            "description": "Optimize hybrid search performance to meet p95 â‰¤ 1.2s total latency and â‰¤350ms reranking latency requirements",
            "dependencies": [
              "70.6"
            ],
            "details": "Implement performance monitoring and optimization for hybrid search pipeline. Add database query optimization with proper indexing strategies. Implement parallel processing for vector and FTS searches. Add caching layers for frequently accessed data. Create performance benchmarking suite and load testing framework. Implement latency monitoring with alerts for SLA violations. Optimize Together AI API calls with request batching and connection pooling.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 71,
        "title": "Develop Cloud Run Enrichment Service with Pub/Sub Integration",
        "description": "Create containerized Cloud Run service for scalable candidate enrichment processing with Pub/Sub orchestration",
        "details": "Build Cloud Run candidate-enricher service in Python with aiohttp for processing Pub/Sub messages. Create candidate-process-requests topic for message queuing. Implement idempotent processing with upsert by candidate_id. Set up Cloud Scheduler for batch processing and nightly updates. Configure horizontal scaling and concurrency controls. Integrate with GCS for resume storage (encrypted with signed URLs) and Firestore for profile persistence. Include processing resilience with retry logic and dead letter queues.",
        "testStrategy": "Container deployment tests, Pub/Sub message processing validation, idempotency testing, scaling behavior verification, error handling and retry logic tests",
        "priority": "medium",
        "dependencies": [
          68
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Complete Dockerfile and Container Configuration",
            "description": "Finalize the Dockerfile in cloud_run_worker/ directory with proper Python dependencies, environment setup, and container optimization for Cloud Run deployment",
            "dependencies": [],
            "details": "Complete the existing Dockerfile with multi-stage build, install required dependencies (aiohttp, google-cloud-pubsub, google-cloud-firestore, google-cloud-storage), set proper environment variables, configure health checks, and optimize for Cloud Run with proper port configuration and startup commands",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Pub/Sub Message Handler Integration",
            "description": "Complete the pubsub_handler.py to process messages from candidate-process-requests topic with proper message acknowledgment and error handling",
            "dependencies": [
              "71.1"
            ],
            "details": "Enhance existing pubsub_handler.py to subscribe to candidate-process-requests topic, implement message parsing and validation, add proper message acknowledgment/nack logic, integrate with the enrichment processor from Task 68, and include structured logging for message processing events",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Add Idempotent Processing with Candidate ID Upsert",
            "description": "Implement idempotency logic to prevent duplicate processing and ensure upsert operations by candidate_id in Firestore",
            "dependencies": [
              "71.2"
            ],
            "details": "Create idempotency mechanism using candidate_id as unique key, implement upsert logic for Firestore document updates, add processing state tracking to prevent duplicate enrichment, include timestamp-based freshness checks, and ensure atomic operations for data consistency",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Configure Horizontal Scaling and Concurrency Controls",
            "description": "Set up Cloud Run service configuration for automatic scaling, concurrency limits, and resource allocation optimization",
            "dependencies": [
              "71.3"
            ],
            "details": "Configure Cloud Run service YAML with appropriate CPU/memory limits, set max instances and concurrency per instance, implement graceful shutdown handling, configure request timeout settings, and set up proper health check endpoints for scaling decisions",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Set Up Cloud Scheduler and Implement Retry/DLQ Logic",
            "description": "Create Cloud Scheduler jobs for batch processing and implement comprehensive retry logic with dead letter queue handling",
            "dependencies": [
              "71.4"
            ],
            "details": "Set up Cloud Scheduler for nightly batch processing jobs, create dead letter queue topic for failed messages, implement exponential backoff retry logic, configure maximum retry attempts, add monitoring and alerting for DLQ messages, and create batch processing endpoints for scheduled operations",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 72,
        "title": "Implement Firebase Authentication and Authorization System",
        "description": "Set up Firebase Authentication with Google Sign-In and role-based access control for Ella recruiters and admins",
        "details": "Configure Firebase Authentication with Google Sign-In restricted to Ella employees. Implement allowed_users collection in Firestore with email domain allowlist and role management (admin/recruiter). Create Firestore security rules enforcing access controls. Build admin-only user management interface with addAllowedUser, removeAllowedUser, listAllowedUsers, and setAllowedUserRole Cloud Functions. Set up role-based UI routing and component access controls. Phase I supports Ella recruiters and admin only.",
        "testStrategy": "Authentication flow tests, authorization rule validation, role-based access testing, admin interface functionality tests, security rule enforcement verification",
        "priority": "medium",
        "dependencies": [],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure Firebase Authentication with Google Sign-In and Domain Restrictions",
            "description": "Set up Firebase Authentication with Google Sign-In provider and implement domain restrictions to allow only Ella employees (@ella.com.br) to authenticate",
            "dependencies": [],
            "details": "Configure Firebase Auth settings in Firebase Console to enable Google Sign-In provider. Implement domain restriction logic in authentication flow to validate user email domains against allowed list (@ella.com.br). Update existing Auth components in headhunter-ui/src/components/Auth/ to enforce domain validation. Set up proper error handling for unauthorized domain attempts.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create allowed_users Firestore Collection with Role Management Schema",
            "description": "Design and implement the allowed_users collection in Firestore with proper schema for email allowlist and role management",
            "dependencies": [
              "72.1"
            ],
            "details": "Create allowed_users collection schema with fields: email, role (admin/recruiter), created_at, updated_at, created_by. Set up proper indexing for efficient queries. Initialize collection with initial admin users. Implement data validation rules for role assignments and email format validation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Firestore Security Rules for Role-Based Access Control",
            "description": "Create comprehensive Firestore security rules that enforce role-based access control across all collections",
            "dependencies": [
              "72.2"
            ],
            "details": "Write security rules that check user authentication and role permissions from allowed_users collection. Implement rules for admin-only access to user management functions and recruiter access to candidate data. Ensure proper read/write permissions based on user roles. Test security rules thoroughly to prevent unauthorized access.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build Admin User Management Interface with Cloud Functions Backend",
            "description": "Complete the admin UI components for user management and integrate with Cloud Functions for backend operations",
            "dependencies": [
              "72.2",
              "72.3"
            ],
            "details": "Enhance existing admin-users.ts Cloud Functions with addAllowedUser, removeAllowedUser, listAllowedUsers, and setAllowedUserRole operations. Build admin-only UI components for user management interface with proper role-based routing. Implement real-time updates for user list changes. Add proper error handling and success notifications for all user management operations.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 73,
        "title": "Build React Search Interface with Job Description Input",
        "description": "Create React web application for job description input and candidate search results display with explainability features",
        "details": "Develop React SPA hosted on Firebase Hosting with job description input interface supporting EN/PT-BR. Implement search results display showing top-20 candidates with 'Why match' evidence bullets. Create candidate detail pages with skill maps, confidence scores, LinkedIn URLs, and resume freshness indicators. Include signed URL integration for secure resume access from GCS. Implement responsive design with accessibility considerations. Add search filters and result pagination. Integrate with skillAwareSearch and getCandidateSkillAssessment Cloud Functions.",
        "testStrategy": "Component unit tests, integration tests with search API, accessibility testing, responsive design validation, user flow testing for search and candidate detail views",
        "priority": "high",
        "dependencies": [
          70,
          72
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Job Description Input Component with Multilingual Support",
            "description": "Develop React component for job description input with EN/PT-BR language support and validation",
            "dependencies": [],
            "details": "Build JobDescriptionInput.tsx component with textarea for job description, language selector (EN/PT-BR), character count, validation feedback, and submit functionality. Include form state management, input sanitization, and responsive design. Add placeholder text and help tooltips in both languages. Implement debounced validation and error handling for API integration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Build Search Results Display with Pagination",
            "description": "Implement search results component displaying top-20 candidates with pagination and filtering",
            "dependencies": [
              "73.1"
            ],
            "details": "Create SearchResults.tsx component extending existing SkillAwareCandidateCard.tsx. Display candidate cards with basic info, match scores, and preview of 'Why match' bullets. Implement pagination controls, results counter, and loading states. Add search filters for experience level, location, and skills. Include sort options and empty state handling. Integrate with skillAwareSearch Cloud Function.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement Candidate Detail Pages with Skill Visualization",
            "description": "Create detailed candidate profile pages with skill maps, confidence scores, and comprehensive information display",
            "dependencies": [
              "73.2"
            ],
            "details": "Build CandidateDetail.tsx component with skill visualization charts, confidence score indicators, experience timeline, education details, and contact information. Include LinkedIn URL integration, resume freshness indicators, and downloadable resume access. Implement skill mapping with visual confidence levels and evidence tooltips. Add navigation breadcrumbs and back-to-results functionality.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add Explainability Features with 'Why Match' Evidence Display",
            "description": "Implement explainability components showing detailed match reasoning and evidence bullets",
            "dependencies": [
              "73.2"
            ],
            "details": "Create ExplainabilityPanel.tsx component displaying 'Why match' evidence bullets with skill-specific reasoning. Show match confidence scores, evidence sources, and detailed explanations for each skill match. Include expandable sections for different match categories (technical skills, experience, education). Implement evidence highlighting and cross-references to candidate profile sections.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Integrate Signed URL Generation for Secure Resume Access",
            "description": "Implement secure resume access using signed URLs from Google Cloud Storage with proper authentication",
            "dependencies": [
              "73.3"
            ],
            "details": "Create ResumeAccess.tsx component with secure download functionality using signed URLs from GCS. Implement authentication checks, URL generation requests, and secure file access. Add resume preview modal, download progress indicators, and error handling for expired or invalid URLs. Include file type validation and virus scanning integration. Ensure proper cleanup of temporary URLs.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement Responsive Design and Accessibility Features",
            "description": "Apply responsive design principles and accessibility standards across all React components",
            "dependencies": [
              "73.1",
              "73.2",
              "73.3",
              "73.4",
              "73.5"
            ],
            "details": "Implement responsive breakpoints for mobile, tablet, and desktop views across all components. Add ARIA labels, keyboard navigation support, screen reader compatibility, and focus management. Include high contrast mode support, font scaling options, and touch-friendly interactions. Perform accessibility audits using axe-core and implement WCAG 2.1 AA compliance. Add loading states and error boundaries for better UX.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 74,
        "title": "Develop Cloud Functions API Layer",
        "description": "Create comprehensive Cloud Functions for CRUD operations, search, and candidate management with proper error handling",
        "details": "Implement Cloud Functions v2 for candidates-crud.ts, jobs-crud.ts, file-upload-pipeline.ts, vector-search.ts, and generate-embeddings.ts. Create callable functions for skillAwareSearch, getCandidateSkillAssessment, and admin user management. Implement processUploadedProfile trigger for GCS uploads. Add comprehensive error handling, input validation, and response formatting. Ensure functions support the compliance fields and exportable processing register requirements. Include audit logging for sensitive operations.",
        "testStrategy": "Function unit tests, API endpoint testing, error handling validation, security testing for callable functions, integration tests with frontend, performance testing for function cold starts",
        "priority": "medium",
        "dependencies": [
          70,
          72
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement CRUD Functions with Validation",
            "description": "Complete candidates-crud.ts and jobs-crud.ts Cloud Functions with comprehensive input validation, compliance field support, and proper response formatting",
            "dependencies": [],
            "details": "Enhance existing CRUD functions in functions/src/ to include robust input validation, support for compliance fields (exportable processing register requirements), proper error responses, and standardized data formatting. Ensure functions handle create, read, update, delete operations for both candidates and jobs collections with appropriate Firestore security rules integration.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create File Upload Pipeline Function",
            "description": "Implement file-upload-pipeline.ts with GCS trigger for processUploadedProfile and secure file handling",
            "dependencies": [],
            "details": "Build Cloud Function triggered by GCS uploads to process candidate profile documents. Implement secure file validation, virus scanning integration, metadata extraction, and trigger downstream enrichment processes. Include support for multiple file formats (PDF, DOC, DOCX) and proper error handling for corrupted or invalid files.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Build Vector Search and Embeddings Functions",
            "description": "Complete vector-search.ts and generate-embeddings.ts functions for semantic search capabilities",
            "dependencies": [],
            "details": "Implement vector search functionality using Firestore vector search capabilities and generate embeddings for candidate profiles and job descriptions. Include similarity scoring, relevance ranking, and integration with the enrichment pipeline. Ensure functions support batch processing and efficient vector operations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Add Callable Functions for Skill Assessment",
            "description": "Implement skillAwareSearch and getCandidateSkillAssessment callable functions with admin user management",
            "dependencies": [
              "74.1",
              "74.3"
            ],
            "details": "Create callable Cloud Functions for skill-aware candidate search and detailed skill assessments. Include admin user management functions for user role assignment and permissions. Implement proper authentication, authorization checks, and rate limiting. Ensure functions return structured skill analysis with confidence scores and recommendations.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Implement Error Handling and Audit Logging",
            "description": "Add comprehensive error handling, audit logging, and monitoring across all Cloud Functions",
            "dependencies": [
              "74.1",
              "74.2",
              "74.3",
              "74.4"
            ],
            "details": "Implement centralized error handling with proper HTTP status codes, structured error responses, and retry mechanisms. Add audit logging for all sensitive operations including data access, modifications, and admin actions. Include monitoring hooks, performance metrics collection, and integration with Cloud Logging for observability.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 75,
        "title": "Implement Resume Storage and LinkedIn URL Management",
        "description": "Set up secure resume storage in GCS with encryption and LinkedIn URL extraction and management",
        "details": "Configure GCS buckets (*-raw-csv, *-raw-json, *-profiles) with encryption for secure resume storage. Implement signed URL generation for secure resume access with appropriate expiration. Create LinkedIn URL extraction from CSV data and resume text using regex patterns. Store linkedin_url field in candidate profiles. Implement resume freshness tracking with resume_updated_at timestamps and UI badges (Recent/Stale/Very stale). Add re-upload functionality with version control. No automated LinkedIn scraping in MVP - manual updates only.",
        "testStrategy": "GCS encryption validation, signed URL generation and expiration tests, LinkedIn URL extraction accuracy tests, resume versioning functionality tests, file upload security testing",
        "priority": "medium",
        "dependencies": [
          71
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Configure GCS Buckets with Encryption and IAM",
            "description": "Set up GCS buckets (*-raw-csv, *-raw-json, *-profiles) with proper encryption configuration and IAM permissions for secure resume storage",
            "dependencies": [],
            "details": "Configure three GCS buckets with appropriate naming conventions. Enable server-side encryption with customer-managed encryption keys (CMEK) or Google-managed keys. Set up proper IAM roles and permissions for service accounts. Configure bucket lifecycle policies for data retention. Implement proper access controls and security policies.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Implement Signed URL Generation with Expiration Logic",
            "description": "Create secure signed URL generation system for resume access with appropriate expiration times and access controls",
            "dependencies": [
              "75.1"
            ],
            "details": "Build signed URL generation service that creates time-limited access URLs for resume files. Implement configurable expiration times (e.g., 1 hour for viewing, 24 hours for downloads). Add URL validation and revocation capabilities. Integrate with existing file-upload-pipeline.ts. Include proper error handling and logging for URL generation failures.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Create LinkedIn URL Extraction Using Regex Patterns",
            "description": "Implement LinkedIn URL extraction from CSV data and resume text using regex patterns and store in candidate profiles",
            "dependencies": [],
            "details": "Develop regex patterns to extract LinkedIn URLs from various formats (linkedin.com/in/, linkedin.com/pub/, etc.). Implement extraction logic for both CSV import data and resume text content. Add linkedin_url field to candidate profile schema. Create validation for extracted URLs and handle edge cases. Integrate with existing resume_extractor.py for text-based extraction.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build Resume Freshness Tracking with Timestamps and UI Integration",
            "description": "Implement resume freshness tracking system with resume_updated_at timestamps, UI badges, and re-upload functionality with version control",
            "dependencies": [
              "75.2"
            ],
            "details": "Add resume_updated_at timestamp field to track resume upload/update times. Implement freshness categorization logic (Recent: <30 days, Stale: 30-90 days, Very stale: >90 days). Create UI badge components to display freshness status. Build re-upload functionality with version control to maintain resume history. Add automatic timestamp updates on file uploads and manual profile updates.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 76,
        "title": "Build Pre-Interview Analysis Feature",
        "description": "Implement on-demand pre-interview analysis generation with caching and structured output for candidate assessment",
        "details": "Create preInterviewAnalysis.generate and preInterviewAnalysis.get callable functions. Implement analysis generation using Together AI to produce structured output with summary, strengths, potential_red_flags, and signal analysis (promotion_velocity, company_pedigree, stability_pattern, promotion_pattern, academics_strength). Include evidence array with source references. Implement caching with 14-day TTL and invalidation on profile changes. Store analysis under candidates/{id}.pre_interview_analysis with model_version and generated_at metadata. Target â‰¥99% valid JSON output.",
        "testStrategy": "Analysis generation accuracy tests, caching mechanism validation, TTL and invalidation testing, JSON schema validation, performance testing for on-demand generation",
        "priority": "low",
        "dependencies": [
          68,
          74
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Together AI Analysis Generation Function",
            "description": "Implement the core analysis generation function using Together AI to process candidate profiles and generate structured pre-interview analysis",
            "dependencies": [],
            "details": "Create preInterviewAnalysis.generate function that interfaces with Together AI API. Implement prompt engineering to extract structured insights from candidate profiles. Handle API rate limits, retries, and error scenarios. Ensure robust JSON parsing and validation of AI responses. Target â‰¥99% valid JSON output through prompt optimization and response validation.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Design Structured Output Schema",
            "description": "Define and implement the structured output schema for pre-interview analysis including summary, strengths, red flags, and signal analysis",
            "dependencies": [],
            "details": "Create comprehensive schema for analysis output including summary, strengths array, potential_red_flags array, and signal analysis (promotion_velocity, company_pedigree, stability_pattern, promotion_pattern, academics_strength). Design evidence array structure with source references. Implement JSON schema validation and type definitions. Ensure schema supports interview preparation requirements and maintains consistency across analyses.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement 14-Day TTL Caching Mechanism",
            "description": "Build caching system with 14-day time-to-live for pre-interview analyses to optimize performance and reduce API costs",
            "dependencies": [
              "76.1",
              "76.2"
            ],
            "details": "Implement caching layer storing analyses under candidates/{id}.pre_interview_analysis path. Add metadata fields including model_version, generated_at timestamp, and TTL expiration. Create cache retrieval logic that checks expiration and validity. Implement automatic cleanup of expired cache entries. Ensure cache performance meets p95 â‰¤ 1.2s requirement.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build Cache Invalidation on Profile Changes",
            "description": "Implement cache invalidation mechanism that triggers when candidate profiles are updated to ensure analysis freshness",
            "dependencies": [
              "76.3"
            ],
            "details": "Create profile change detection system that monitors candidate data updates. Implement cache invalidation triggers for relevant profile changes (work experience, education, skills updates). Design selective invalidation to avoid unnecessary cache clearing. Add logging for cache invalidation events for monitoring and debugging. Ensure invalidation happens atomically with profile updates.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Create PreInterviewAnalysis Callable Functions with Error Handling",
            "description": "Build the public API functions preInterviewAnalysis.generate and preInterviewAnalysis.get with comprehensive error handling and validation",
            "dependencies": [
              "76.1",
              "76.2",
              "76.3",
              "76.4"
            ],
            "details": "Create preInterviewAnalysis.generate callable function with input validation, rate limiting, and error handling. Implement preInterviewAnalysis.get function for cached analysis retrieval with fallback to generation. Add comprehensive error handling for API failures, invalid inputs, and system errors. Implement logging and monitoring for function usage and performance. Ensure functions integrate seamlessly with existing authentication and authorization systems.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      },
      {
        "id": 77,
        "title": "Implement Compliance and Monitoring Systems",
        "description": "Set up compliance tracking, monitoring, and exportable processing register for GDPR and data governance requirements",
        "details": "Implement compliance fields (legal_basis, consent_record, transfer_mechanism) in database schema and API responses. Create exportable processing register functionality for audit requirements. Set up monitoring for p95 â‰¤ 1.2s performance, â‰¥95% valid JSON rate, and <1% auto-repair/quarantine metrics. Implement audit logging for all data access and processing operations. Create health check endpoints and system status monitoring. Add data retention policies and deletion capabilities for compliance. Configure alerting for performance and error rate thresholds.",
        "testStrategy": "Compliance field validation, processing register export testing, performance monitoring accuracy, audit log completeness testing, data retention policy validation, alerting system testing",
        "priority": "medium",
        "dependencies": [
          67,
          71,
          74
        ],
        "status": "pending",
        "subtasks": [
          {
            "id": 1,
            "title": "Add compliance fields to database schemas and API responses",
            "description": "Implement compliance fields (legal_basis, consent_record, transfer_mechanism) in database schemas and ensure they are included in all relevant API responses",
            "dependencies": [],
            "details": "Update database schemas to include GDPR compliance fields. Modify API response structures to include compliance metadata. Ensure all data processing operations capture and store required compliance information.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 2,
            "title": "Create exportable processing register functionality",
            "description": "Build functionality to generate and export processing register reports for GDPR audit requirements",
            "dependencies": [
              "77.1"
            ],
            "details": "Implement processing register data collection and aggregation. Create export functionality supporting multiple formats (CSV, JSON, PDF). Include all required GDPR processing register fields and metadata.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 3,
            "title": "Implement performance monitoring for p95 latency tracking",
            "description": "Set up comprehensive performance monitoring system to track p95 latency with target of â‰¤ 1.2s",
            "dependencies": [],
            "details": "Implement latency measurement across all API endpoints. Set up p95 percentile calculation and tracking. Create performance dashboards and historical trend analysis. Configure automated performance reporting.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 4,
            "title": "Build JSON validation rate monitoring system",
            "description": "Create monitoring system to track JSON validation rates with target of â‰¥95% valid JSON rate and <1% auto-repair/quarantine metrics",
            "dependencies": [],
            "details": "Implement JSON validation monitoring across all data processing pipelines. Track validation success rates, auto-repair attempts, and quarantine events. Create metrics collection and reporting for data quality monitoring.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 5,
            "title": "Set up comprehensive audit logging system",
            "description": "Implement audit logging for all data access and processing operations to meet compliance requirements",
            "dependencies": [
              "77.1"
            ],
            "details": "Enhance existing audit-logger.ts with comprehensive logging capabilities. Implement audit trails for all data access, modifications, and processing operations. Ensure audit logs include user context, timestamps, and operation details.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 6,
            "title": "Implement data retention policies and deletion capabilities",
            "description": "Build data retention policy enforcement and automated deletion capabilities for GDPR compliance",
            "dependencies": [
              "77.1",
              "77.5"
            ],
            "details": "Create data retention policy configuration and enforcement mechanisms. Implement automated data deletion workflows based on retention periods. Build data subject deletion capabilities for GDPR right to erasure compliance.",
            "status": "pending",
            "testStrategy": ""
          },
          {
            "id": 7,
            "title": "Configure alerting for performance and error thresholds",
            "description": "Set up comprehensive alerting system for performance metrics, error rates, and compliance violations",
            "dependencies": [
              "77.3",
              "77.4"
            ],
            "details": "Configure alerting rules for performance threshold violations, error rate spikes, and compliance issues. Implement multi-channel alerting (email, Slack, SMS). Create escalation policies and alert fatigue prevention mechanisms.",
            "status": "pending",
            "testStrategy": ""
          }
        ]
      }
    ],
    "metadata": {
      "created": "2025-09-05T18:18:48.859Z",
      "updated": "2025-09-14T14:40:45.424Z",
      "description": "Tasks for master context"
    }
  }
}