# Headhunter v2.0 – Cloud Processing with Together AI

## Overview
Headhunter transforms Ella Executive Search's historical candidate database into an intelligent, semantic search engine using a cloud processing pipeline powered by Together AI. The system analyzes unstructured candidate data (resumes, recruiter comments, experience text) to produce enriched, structured profiles and semantic search embeddings, then serves ranked results to recruiters.

Key change: AI analysis and enrichment run via Together AI’s hosted models (single‑pass Qwen 2.5 32B Instruct) from Python processors, with results streamed directly to Firebase Firestore. Cloud Functions provide ingestion, CRUD APIs, search, and embedding generation. Local‑only processing is for development only; production processing uses Together AI.

## Core Features
- **Cloud AI Processing (Together AI)**: Python async processors call Together AI chat completion API with recruiter-grade prompts to analyze resumes and recruiter comments, returning structured JSON analysis.
- **Streaming to Firebase**: Processed/enhanced profiles are streamed directly to Firestore (candidates collection) and optionally uploaded to Cloud Storage to trigger Cloud Functions.
- **Embeddings + Semantic Search**: Cloud Functions generate text embeddings for profiles and provide vector-similarity search endpoints. Current implementation uses Vertex AI text-embedding-004 (with a deterministic fallback in development). Migration to Together AI embeddings is feasible.
- **Semantic search UI**: Secure web app where recruiters paste a full job description and receive a ranked list of 10–20 candidates with AI summary and “Why they’re a match”.

## User Experience
**Persona: Alex (Senior Recruiter)**
- Goals: Quickly surface the best candidates; fill roles faster; deliver high client value; maintain data privacy.
- Frustrations: Time-consuming keyword searches; fear of missing the "perfect" past candidate; concerns about data security with cloud AI.

**Key flow**
1) Paste JD → 2) Submit search → 3) View ranked candidates → 4) Read "Why they're a match" → 5) Shortlist.

**UX considerations**: Minimal inputs, fast results, clear rationale bullets, accessible UI on Firebase Hosting, confidence in data privacy.

## Technical Architecture
**System components**
- **Cloud LLM Processing (Together AI)**: Single-pass enrichment with Qwen 2.5 32B Instruct (configurable). Python 3.10+ async processors (aiohttp) call Together AI chat completions to produce enriched candidate JSON that includes explicit skills and inferred skills with confidence and evidence. Entrypoints include:
  - `scripts/together_ai_processor.py`
  - `scripts/enhanced_together_ai_processor.py`
  - `scripts/firebase_streaming_processor.py`
  - `scripts/together_ai_firestore_processor.py`
  - `scripts/intelligent_skill_processor.py` (preferred path; adds analysis_confidence and quality_flags)
- **Data Ingestion**: Two supported paths:
  - Direct Firestore streaming from processors (primary at scale).
  - Cloud Storage upload of profile JSON to `profiles/*.json`, triggering Cloud Functions (`processUploadedProfile`).
- **Storage & API Layer**: Firebase Cloud Functions (Node/TS) provide:
  - CRUD APIs (`candidates-crud.ts`, `jobs-crud.ts`)
  - File upload pipeline (`file-upload-pipeline.ts`)
  - Enrichment orchestration and Firestore writes (`index.ts`)
  - Semantic search + embeddings (`vector-search.ts`, `generate-embeddings.ts`)
- **Embeddings**: `VectorSearchService` currently generates embeddings via Vertex AI `text-embedding-004`. No implicit mock/deterministic fallback is used in production; for development only, you may explicitly set `EMBEDDING_PROVIDER=local`. Canonical collection is `candidate_embeddings` for similarity search. Unify any writers previously using `embeddings/`.
- **Frontend**: React app on Firebase Hosting calling secure Cloud Functions.

**Data models**
- **AI-Generated Candidate Profiles** (via Together AI, single-pass):
  - personal_details: name, seniority_level, years_of_experience, location
  - education_analysis: degrees, quality_score, relevance
  - experience_analysis: companies, current_role, career_progression, industry_focus
  - technical_assessment: primary_skills, expertise_level
  - market_insights: salary_range, market_demand, placement_difficulty
  - cultural_assessment / recruiter_recommendations: strengths, red_flags, ideal_roles, target_companies
  - executive_summary: one_line_pitch, overall_rating, recommendation
  - skill_inference: explicit_skills (100%), inferred_skills with confidence (0–100) and evidence/reasoning
  - analysis_confidence: overall signal confidence [0,1] (low_content demotion)
  - processing_metadata: timestamp, processor, model, version
- **Search-Ready Profiles**: Flattened fields for querying (years_experience, current_role, primary_skills, overall_rating, search_keywords, etc.) plus embedding vectors.

**APIs & integrations**
- Together AI Inference API (`https://api.together.xyz/v1/chat/completions`) – default Stage 1 model: Qwen 2.5 32B Instruct (env: `TOGETHER_MODEL_STAGE1`)
- Firebase Admin/Client SDKs (Firestore, Storage, Auth)
- Cloud Functions v2 (HTTP/callable + Storage triggers)
- Vertex AI embeddings (current implementation) with plan to evaluate Together AI embeddings

**Infrastructure requirements**
- GCP project with Firestore and Cloud Storage enabled (`headhunter-ai-0088`)
- Firebase Hosting and Functions with service account
- Together AI API key in environment (`TOGETHER_API_KEY`)
- Optional: local dev credentials for Firebase Admin

## Development Roadmap

### MVP requirements
- **Together AI Processing (Single Pass)**: Analyze candidates via Qwen 2.5 32B Instruct and produce structured JSON including skill probabilities and evidence. Configure via env (`TOGETHER_MODEL_STAGE1`).
- **Firestore Streaming**: Stream processed profiles to `candidates/` with flattened fields for search; optionally store full enriched profiles in `enriched_profiles/`.
- **Embeddings & Search**: Generate embeddings for profiles and expose a unified semantic search endpoint that blends vector similarity with structured skill signals and experience. Keep Vertex AI embedding service initially; evaluate Together AI embeddings for parity/cost.
- **Quality Validation**: Add schema validation and JSON parsing guards; retry on malformed outputs.
- **Secure Web UI**: JD input → ranked results with rationale, authenticated via Firebase.

### Future enhancements
- Replace Vertex AI embeddings with Together AI embeddings where advantageous.
- Advanced filters, saved searches, export lists.
- Real-time ingestion pipeline for ongoing candidate updates.
- Organization multi-tenancy and role-based access (RBAC) expansion.

### Scope guidance
- Prioritize stable, scalable batch processing + search relevance; avoid regressions in parsing/validation.

## Logical Dependency Chain
- **Foundation**: Configure Together AI API access (env keys, model); enable Firestore/Storage; set up Firebase Hosting/Functions.
- **Data path**: Single-pass Together AI analysis → Structured profile JSON (with skills + confidence) → Firestore (and/or GCS → Cloud Function) → Embeddings.
- **Search path**: Embedding generation → ANN recall (pgvector planned) → Re-rank with structured skill probabilities and experience → Ranked results with rationale in UI.
- **Fast-path to usable demo**: Process a sample set via Together AI → Stream to Firestore → Generate embeddings → Minimal UI search.

## Risks and Mitigations
- **API reliability/latency**: Add retries, backoff, and circuit breakers around Together AI requests; batch and rate-limit.
- **JSON parsing robustness**: Strip code fences, validate against schema, retry with repair prompts; quarantine malformed results.
- **Cost control**: Batch sizes, concurrency caps, and MAX_ESTIMATED_COST safeguards (see `scripts/.env.template`); per-run confirmation flags.
- **Data privacy**: Minimize PII in prompts; store only required fields; enforce Firebase security rules; audit logging for access.
- **Embedding consistency**: Until Together embeddings are adopted, keep stable Vertex AI embeddings with deterministic fallback for dev.
- **Scalability**: Horizontal scale by increasing concurrent requests and batching; streaming writes to Firestore in batches.
 - **Low-certainty cases**: Tag low-content profiles with low `analysis_confidence` and demote in ranking rather than running a second pass.

## Goals & Success Metrics
- Time-to-Longlist < 30 minutes
- > 5 searches per recruiter per week
- Satisfaction > 4.5/5
- Predictable cost per processed candidate with guardrails
- > 95% valid JSON parse rate from Together AI responses
 - ≤ 1% JSON repair/quarantine rate for production pipelines
 - < 0.5s re-rank latency for top‑K ANN results (when pgvector is enabled)

## Access Control & CRUD (Phase after Processing)

- Authentication: Firebase Authentication with Google Sign-In (Ella employees only initially)
- Authorization: Simple allowlist of email domains/users in Firestore (`allowed_users/{email}` with role) and an Admin-only page to manage access
- Roles (initial):
  - admin: manage users, view all data, run maintenance jobs
  - recruiter: search candidates, view details, save/export lists
  - viewer (future): read-only search and view results
- CRUD scope:
  - Candidates: view enhanced profiles, edit recruiter notes/tags, bookmark

## Product UX

- People Search (specific person): Search by name or LinkedIn URL; opens Candidate Page.
- Job Search: Paste JD → up to 50 results (expandable by 50). Rows are minimal; click row to open Candidate Page.
- Candidate Page: Full Skill Map (explicit + inferred with confidence and “Needs verification” tags), Pre‑Interview Analysis (on‑demand), compact career Timeline, resume Freshness, LinkedIn link.

List row content (minimal): name, current role @ company, years/level, composite score, freshness badge, LinkedIn link, and a small “Low profile depth” badge when applicable.

## Search & Ranking (Unified)

- ANN recall (pgvector planned) unioned with deterministic title/company matches; then composite re‑rank combining skill_match, confidence, vector_similarity, and experience. Apply analysis_confidence demotion with a higher floor when deterministic matches exist. Optional small quota for a “Potential matches (low profile depth)” section to avoid losing sparse profiles.

## LLM Usage

- Stage 1 enrichment (single pass, ingestion/update): Qwen 2.5 32B produces structured profile including skills with confidence/evidence and `analysis_confidence`. Embeddings generated from enriched text.
- Search time: no LLM calls.
- Pre‑Interview Analysis (on‑demand): Generate summary, strengths, potential red flags, and signal chips; cache per candidate/version with TTL; invalidate on profile change.

### Pre‑Interview Analysis (Feature spec)

User stories:
- Recruiters get a concise, structured analysis highlighting promotion velocity, company pedigree, stability pattern, promotion pattern, and academics strength with evidence.

Input context:
- Structured profile (experience timeline, summary), education, recruiter notes (optional).

Output schema stored under `candidates/{id}.pre_interview_analysis`:
- `summary`: string (2–3 sentences)
- `strengths`: string[]
- `potential_red_flags`: string[]
- `signals`: { promotion_velocity, company_pedigree, stability_pattern, promotion_pattern, academics_strength }
- `evidence`: array of { source, ref, snippet }
- `model_version`, `generated_at`

API (planned):
- `preInterviewAnalysis.generate` (callable) → runs, stores, returns analysis
- `preInterviewAnalysis.get` (callable) → returns last stored analysis

Caching:
- TTL ~14 days; invalidate on profile fingerprint/model/prompt version change.

Acceptance criteria:
- ≥99% valid JSON; clear error when disabled/unavailable; no mock outputs.

## Candidate Essentials

- Inferred skills: shown on Candidate Page with confidence; “Needs verification” tags below threshold (default 0.75); evidence tooltips.
- LinkedIn: `linkedin_url` extracted from CSV or regex from resume text; shown in list/detail.
- Freshness: show `resume_updated_at` with badges (Recent/Stale/Very stale); “Re‑upload latest resume” CTA.

## Out of Scope (Now)

- Automated LinkedIn downloads/scraping. Future “Stale Profile Queue” with manual actions only; ToS‑safe.
  - Jobs: create/search job descriptions, attach JD text, view suggested matches
  - Lists: save and share candidate lists per job (internal only for now)
  - Activity log: basic audit trail for edits/exports
- Admin UI (minimal):
  - Add/remove allowed users, set role
  - View system health (Firestore counts, embedding stats)

## Bake-off: Embeddings and Model Choice

- Baseline: Vertex `text-embedding-004` (768-dim, cosine)
- Alternatives: Together AI embeddings (strong retrieval models), open models (e.g., BGE, Nomic, Embedding Gemma via self-hosting)
- Dataset: ~2,000 candidates × 30–50 representative JDs (or candidate-to-candidate similarity)
- Metrics: NDCG@10, MRR@10, Hit@10; latency and cost per 10k embeddings
- Outcome: Standardize on best performer; keep provider pluggable via env

## Development Process (TDD Required)

## Model & Ranking Policy (Decisions)

- Stage 1 Model: Qwen 2.5 32B Instruct on Together AI (single pass). Env: `TOGETHER_MODEL_STAGE1`.
- Optional: larger models for ambiguous cases (future; off by default). No automatic escalation in v2.0.
- Low-Certainty Policy: Compute `analysis_confidence` and demote low-content profiles; still searchable by deterministic fields (name/company).
- Unified Search: One endpoint blends vector similarity (ANN) + skill probabilities + experience into a single rank; UI never uses “two engines”.

## Implementation Tasks (Updated)

1. UI callable wiring: add `skillAwareSearch` & `getCandidateSkillAssessment` exports.
2. JSON repair + schema validation (TDD) integrated in processors; target <1% repair/quarantine.
3. Functions cleanup: remove stray entrypoints, guard/remove Gemini enrichment.
4. Embeddings: standardize collection to `candidate_embeddings`; align rules/indexes.
5. Vector search: implement pgvector service on Cloud Run; re-rank with structured features; SPA integration.
6. Cloud Run worker (optional): containerized Python Pub/Sub worker for throughput scaling.
7. Documentation: update README, Handover (web steps), and Architecture to match decisions.

- All tasks follow Test-Driven Development (see `docs/TDD_PROTOCOL.md`).
- Each task includes: write tests first → implement → tests green → docs update → commit/push → next task.
- CI (future): Add GitHub Actions to enforce tests on PR.

## Orchestration & Resilience

- Buckets: `*-raw-csv`, `*-raw-json`, `*-profiles`
- Pub/Sub: `candidate-process-requests` (1 msg per candidate JSON in GCS)
- Cloud Run: `candidate-enricher` (Python aiohttp) processes messages → Together AI → Firestore
- Embedding Worker: pub/sub-triggered to generate embeddings and upsert to Cloud SQL `pgvector`
- Scheduler: batch enqueues for initial 29k, nightly reprocessing for updated resumes
- Idempotency: Upsert by `candidate_id` across Firestore and pgvector

## Permissions & Security

- Minimize PII in prompts; redact where possible
- Firestore rules enforce `allowed_users`
- Admin actions behind role checks; audit logs for sensitive operations

## Deliverables (Tracked by Task Master)

1. Cloud Run worker (Together AI enrichment)
2. Pub/Sub topic + Scheduler for batch and nightly updates
3. Cloud SQL (Postgres + pgvector) provisioning and schema
4. Embedding provider adapter (env-driven: vertex|together|local)
5. Embedding bake-off harness + report
6. React UI search: JD input, ranked results with rationale, filters
7. Auth: Google Sign-In; Firestore allowlist + minimal Admin page
8. Migrate NAS → GCS; one-time 50-candidate test run
9. Remove Gemini enrichment references from Functions; fix mismatch
10. Handover docs kept up-to-date (crash-safe)

## Out of Scope (current)
- Client-facing multi-tenancy and billing
- Real-time ingestion of resumes (beyond current bucket triggers)
- Open-ended generative features in the UI

## User Stories
- **Epic 1 (Together AI Processing)**: 1.1 Configure TOGETHER_API_KEY, 1.2 Implement/validate recruiter-grade prompts, 1.3 Batch process candidates with streaming to Firestore, 1.4 Robust JSON validation and retries.
- **Epic 2 (Storage & API)**: 2.1 Cloud Storage upload pipeline + trigger, 2.2 CRUD/search APIs via Cloud Functions, 2.3 Embedding generation + vector search.
- **Epic 3 (Search Interface)**: 3.1 Secure web access, 3.2 JD input interface, 3.3 Ranked candidate results (10–20), 3.4 “Why they’re a match” rationale generation.

## Privacy & Security Statement
Headhunter prioritizes data privacy by minimizing sensitive content sent to external services and enforcing strict access controls. Candidate analysis is performed via Together AI with guarded prompts and schema-constrained outputs. All persisted data is protected by Firebase Authentication and Firestore security rules. Audit logs capture access and processing events for traceability.
